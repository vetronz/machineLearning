{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics\n",
    "\n",
    "## Bayesian thinking\n",
    "\n",
    "### The eyes and the brain\n",
    "\n",
    "Imagine I enter the classroom by telling you that I have just spotted the Loch Ness monster  in the lake at Silwood Park campus (or Hyde Park).\n",
    "![Loch Ness monster](Images/LochNessMonster.jpg)\n",
    "What does this information tell you on the existence or not of Nessie?\n",
    "\n",
    "In the classic frequentist, or likelihoodist, approach you make some inferences based on all the data that you have observed.\n",
    "The only data that you observe here is me telling you whether or not I saw Nessie.\n",
    "In other words, your inference on whether Nessie exists (at Silwood/Hyde park!) or not will be solely based on such observations.\n",
    "\n",
    "Let's denote $D$ (data) as the set of observations specifying whether I tell you that I saw\n",
    "Nessie ($D=1$) or not ($D=0$).\n",
    "$D$ is our sample space, the set of all possible outcomes of the experiment, and $D = \\{0,1\\}$.\n",
    "We want to make some inferences on the probability that Nessie exists,\n",
    "or that it is true that I saw it (her?).\n",
    "Let's denote this probability as $N$.\n",
    "\n",
    "We can define a likelihood function for $p(D|N)$.\n",
    "For instance, assuming that our observation is $D=1$, we could set $p(D=1|N=0)=0.01$\n",
    "(what are the chances that I did not see Nessie, so it doesn't exist, but I tell I saw it?)\n",
    "and $p(D=1|N=1)=0.90$ (what are the chances that I did see Nessie and tell you I saw it so it exists?).\n",
    "\n",
    "Let's make it slightly more complicated and imagine that even the second and third lecturer\n",
    "of the day tell you that they saw Nessie.\n",
    "Let's assume that $p(D=1|N=0)=0.01$ and $p(D=1|N=1)=0.90$ are valid for each observer/lecturer $l$.\n",
    "\n",
    "Then the log-likelihood of $N=0$ is given by $\\sum_{l=1}^3 log(p(D=1|N=0))=-6.91$ while\n",
    "the log-likelihood of $N=1$ is given by $\\sum_{l=1}^3 log(p(D=1|N=1))=-0.32$.\n",
    "\n",
    "__ TASK__ What is the Maximum Likelihood Estimate (MLE) of $N$?\n",
    "\n",
    "In this very trivial example we maximise the likelihood function for $N$ and obtain $N_{MLE}=1$.\n",
    "The difference in likelihood between $N=1$ and $N=0$ gives us some sort of confidence level.\n",
    "The more data we have pointing towards one \"direction\", the more confident we are\n",
    "of our inferences.\n",
    "Recalling our previous example, with 3 observations of $D=1$ we obtained a\n",
    "likelihood ratio (LR) of $6.59$.\n",
    "With 100 observations of $D=1$ we would obtain a LR of $219.72$.\n",
    "On the other hand, with only 1 observation, the LR would be $2.20$.\n",
    "\n",
    "We can appreciate how our inference on $N$ is driven solely by our observations, and our\n",
    "inference is taken by the likelihood distribution.\n",
    "In a very informal (and possibly wrong) notation, we can write $p(N|D) \\propto p(D|N)$,\n",
    "where we stress the conditional of $N$ on the observed data $D$.\n",
    "\n",
    "An analogy here can explain this concept further.\n",
    "Imagine that in the likelihood approach we use only one visual (or auditive) organ,\n",
    "i.e. our eyes (or ears).\n",
    "However, in real life, we take many decisions based not solely on what we observe but\n",
    "also on some believes of ours.\n",
    "We usually use another organ, the brain, to make inferences on the probability of a particular\n",
    "event to occur.\n",
    "![](Images/EyeBrain.png)\n",
    "Note that in this cartoon the brain is \"blind\", in the sense that it does not observe the data\n",
    "(no arrow pointing to the eye) but its inferences on the event are based on its own believes.\n",
    "\n",
    "Back to the Loch Ness monster case, we can clearly have some believes whether or not Nessie\n",
    "exists not only because I told you I saw it in the campus.\n",
    "This \"belief\" expresses the probability of Nessie existing $p(N)$ unconditional of the data.\n",
    "Our intuition is that the probability of $N=1$ is\n",
    "somehow a joint product of the likelihood (the eyes) and the belief (the brain).\n",
    "Therefore, $p(N|D) \\propto p(D|N)p(N)$.\n",
    "\n",
    "How can we define $p(N)$?\n",
    "This depends on our blind \"belief\" function.\n",
    "If you are a Sci-Fi fan you might be inclined to set a higher probability (e.g. $p(N=0.2)$) than the one a\n",
    "pragmatical and sceptical person would set (e.g. $p(N=0.002)$).\n",
    "As an illustration, let's assume that $p(D=1|N=0)=0.001$ and $p(D=1|N=1)=0.1$.\n",
    "In the \"Sci-Fi brain\", $p(N=1|D=1) \\ p(D=1|N=1)p(N=1) \\propto 0.1*0.2 = 2e-2$.\n",
    "In the \"sceptical brain\", $p(N=1|D=1) \\propto 0.1*0.002 = 2e-4$.\n",
    "Note that these are not proper probabilities (we will see later how to calculate\n",
    "proper probabilities using \"belief\" functions).\n",
    "We can deduct how the choice of a different \"belief\" function can point us to\n",
    "either different conclusions or confidence levels.\n",
    "\n",
    "In statistics, the \"belief\" function (e.g. $p(N)$) is called _prior probability_ and\n",
    "the joint product of the likelihood ($p(D|N)$) and the prior ($p(N)$) is proportional to\n",
    "the _posterior probability_ ($p(N|D)$).\n",
    "The use of posterior probabilities for inferences is called Bayesian statistics.\n",
    "\n",
    "## What, who and why?\n",
    "\n",
    "### What\n",
    "\n",
    "Bayesian statistics is an alternative to classical frequentist approaches, where\n",
    "maximum likelihood estimates (MLE) and hypothesis testing based on \\textit{p}-values\n",
    "are often used.\n",
    "However, there is no definite division between frequentists and Bayesians as, in\n",
    "many modern applications, the approach taken is eclectic.\n",
    "We now discuss further examples where a Bayesian approach seems the more appropriate\n",
    "strategy to adopt for statistics inference.\n",
    "\n",
    "Imagine you have just submitted a manuscript for publication to a\n",
    "peer-reviewed journal.\n",
    "You want to assess its probability of being accepted and published.\n",
    "This assessment may use, for instance, the information regarding the journal's acceptance\n",
    "rate (say 20%), the quality of the study and its relevance to the journal's\n",
    "scope.\n",
    "\n",
    "Your manuscript is accepted and now you want to evaluate the\n",
    "probability that your next manuscript will be accepted.\n",
    "What is the direct estimate of this probability?\n",
    "You had one success over one trial.\n",
    "Therefore, the probability is 100%.\n",
    "However, it looks clear that this estimate is somehow \"wrong\" as it is based on a\n",
    "small sample size and we know that the acceptance rate is anyway smaller than 100%.\n",
    "\n",
    "You can think of the journal's acceptance rate as our prior information.\n",
    "You are then tempted to set a probability of being accepted smaller than 100%.\n",
    "By doing so you are behaving as a Bayesian statistician, as you are adjusting the\n",
    "direct estimate in light of a prior information.\n",
    "Bayesian statistics have the ability to incorporate prior information into an analysis.\n",
    "\n",
    "Suppose you are conducting an experiment of measuring the biodiversity\n",
    "of some species on particular rock shores in Scotland.\n",
    "Specifically, you are collecting the number of different species of\n",
    "algae in 4 different locations across time,\n",
    "over 3 years.\n",
    "\n",
    "| Year | Loc. A | Loc. B | Loc. C | Loc. D |\n",
    "| ---- | ------ | ------ | ------ | ------ |\n",
    "| 2015 | 45     |   54   | 47     |   52   |\n",
    "| 2016 | 41 | n.a. | 43 | 45|\n",
    "| 2017 | 32 | 38 | 37 | 35|\n",
    "\n",
    "Unfortunately, something happened in 2016 for Location B and you do not have data reported.\n",
    "What is a reasonable value for that entry, with no direct information?\n",
    "Would you think that 100 could be an answer?\n",
    "Perhaps 100 is too high since the numbers surrounding the entry may point towards a value of around 45.\n",
    "We could fit a model or take an average to impute the missing data.\n",
    "\n",
    "Now assume that you have access to some data for Location B in 2016.\n",
    "Specifically, you have partial data where you could retrieve biodiversity levels only for a\n",
    "fifth $(1/5)$  of Location B for 2016.\n",
    "You extrapolate such partial value to obtain an estimate which turns out to be 100.\n",
    "Are you willing now to impute missing data with 100, extrapolated from some partial coverage,\n",
    "while before you thought this number was much higher than expected?\n",
    "\n",
    "A more intuitive solution would be to take a sort of weighted average between this direct\n",
    "(but uncertain) measurement (100) and the indirect estimate you used (45, the average of surrouding cells)\n",
    "when there was no information available.\n",
    "Finally, imagine that you can retrieve biodiversity values for half $(1/2)$ of Location B in 2016.\n",
    "If so, then you would like to \"weight\" more such observation than to the previous case where only\n",
    "a fifth of the area was available.\n",
    "Bayesian statistics formalises such integration between direct and indirect information.\n",
    "\n",
    "Let's recapitulate all inference approaches discussed so far:\n",
    "* The _frequentist_ is based on imagining repeated sampling from a particular\n",
    " model, which defines the probability of the observed data conditional on unknown parameters.\n",
    "* The _likelihoodist_ uses the sampling model as the frequentists but all\n",
    " inferences are based on the observed data only.\n",
    "* The _Bayesian_ requires a sampling model (the likelihood) and a prior distribution\n",
    "  on all unknown parameters. The prior and the likelihood are used to compute the\n",
    "  conditional distribution of the unknown parameters given the observed data.\n",
    "* The _Empirical Bayesian_ (EB) allows the observed data to contribute to\n",
    "  defining the prior distribution.\n",
    "![](Images/EyeBrainEB.png)\n",
    "\n",
    "To put it in a different perspective, assuming $D$ is the data and $\\theta$ is your unknown parameter,\n",
    "the frequentist approach conditions on parameters and integrates over the data, $p(D|\\theta)$.\n",
    "On the other hand, the Bayesian approach conditions on the data and integrates over the\n",
    "parameters, $p(\\theta|D)$.\n",
    "\n",
    "Therefore, in Bayesian statistics we derive proper probability distributions of our parameters\n",
    "of interest, rather than deriving a point estimate.\n",
    "In other words, in Bayesian statistics a probability is assigned to a hypothesis,\n",
    "while under a frequentist inference, a hypothesis is tested without being assigned to a probability\n",
    "of occurring.\n",
    "Unlike likelihoodist, Bayesian inferences can \"accept\" the null hypothesis rather\n",
    "than \"fail to reject\" it.\n",
    "Bayesian procedures can also impose parsimony in model choice and avoid further\n",
    "testing for multiple comparisons.\n",
    "\n",
    "### Who\n",
    "\n",
    "Let's now address perhaps the question you are all asking yourself: why is it called Bayesian?\n",
    "It is called after Thomas Bayes (1701–1761), an English statistician, philosopher and Presbyterian minister.\n",
    "Thomas Bayes never published his famous accomplishment in statistics.\n",
    "His notes were edited and published after his death.\n",
    "He studied logic and theology and at the age of 33 he became a minister in a village in Kent.\n",
    "Only in his later years Thomas Bayes took a deep interest in probability.\n",
    "![](Images/ThomasBayes.jpeg)\n",
    "\n",
    "The general interpretation of statistical inference called Bayesian was in reality pioneered\n",
    "by Pierre-Simon Laplace.\n",
    "In fact, some argue that Thomas Bayes intended his results in a very limited way than modern\n",
    "Bayesians would intend them.\n",
    "In the special case Thomas Bayes presented, the prior and posterior distributions were Beta\n",
    "distributions and the data came from Bernoulli trials.\n",
    "Interestingly, early Bayesian inference was called \"inverse probability\", because it infers\n",
    "backwards from observations to parameters.\n",
    "![](Images/Laplace.jpg)\n",
    "\n",
    "### Why\n",
    "\n",
    "The recent explosion of interest in Bayesian methods for data analysis is mainly because:\n",
    "* of the increased computing power over the last years;\n",
    "* they have good frequentist properties;\n",
    "* their answers are more easily interpretable by non-specialists;\n",
    "* they are already implemented in software packages.\n",
    "We will be able to appreciate these points later on after discussing some features and\n",
    "properties of Bayesian statistics.\n",
    "\n",
    "Bayesian statistics is used in many topics in life sciences, such as genetics (e.g. fine-mapping of disease-susceptibility genes), ecology (e.g. agent-based models), evolution (e.g. inference of phylogenetic trees), bioinformatics (e.g. base calling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian concepts\n",
    "\n",
    "We will now formalise Bayesian equations and provide case studies where such approach\n",
    "has been successfully used in life sciences.\n",
    "\n",
    "### Bayes' theorem\n",
    "\n",
    "Suppose we have a random variable $Y$.\n",
    "Then $f(\\vec{y}|\\vec{\\theta})$ is a probability distribution representing the sampling\n",
    "model for the observed data $\\vec{y}=(y_1,y_2,...,y_n)$ given a vector of unknown\n",
    "parameters $\\vec{\\theta})$.\n",
    "The distribution $f(\\vec{y}|\\vec{\\theta})$ is often called the _likelihood_ and\n",
    "sometimes written as $L(\\vec{\\theta};\\vec{y})$.\n",
    "For the sake of simplicity, we assume $y$ and $\\theta$ to be vectors, and thus replacing notations $\\vec{y}$ and $\\vec{\\theta}$.\n",
    "\n",
    "We know that $L(\\theta;y)$ is not a probability distribution\n",
    "for $\\theta$ given $y$.\n",
    "Therefore $\\int L(\\theta;y)d\\theta$ is not necessarily equal to\n",
    "$1$ or even finite.\n",
    "Nevertheless it is possible to find the value of $\\theta$ that maximises\n",
    "the likelihood function.\n",
    "In other words we can calculate a _maximum likelihood estimate_ (MLE)\n",
    "for $\\theta$, as\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta}=argmax_{\\theta}L(\\theta;y)\n",
    "\\end{equation}\n",
    "\n",
    "In Bayesian statistics, $\\theta$ is not a fixed (although unknown) parameter\n",
    "but a random quantity.\n",
    "This is done by adopting a probability distribution, called _prior distribution_,\n",
    "for ${\\theta}$ that contains any information we have about ${\\theta}$ not\n",
    "related to the data ${y}$.\n",
    "\n",
    "Under this approach, inference on ${\\theta}$ is then based on its _posterior distribution_ given by\n",
    "\\begin{equation}\n",
    "    P({\\theta}|{y}) = \\frac{f({y}|{\\theta})\\pi({\\theta})}{m({y})} \n",
    "                        = \\frac{f({y}|{\\theta})\\pi({\\theta})}{\\int f({y}|{\\theta}) \\pi({\\theta}) d{\\theta}}\n",
    "\\end{equation}\n",
    "This formula is known as _Bayes' Theorem_.\n",
    "The posterior probability is simply the product of the likelihood and the prior,\n",
    "normalised so that integrates to $1$.\n",
    "The posterior distribution is therefore a proper (or legitimate) probability distribution.\n",
    "\n",
    "Let's prove this Theorem using a concrete example.\n",
    "We can also visualise probabilities using Venn diagrams (a useful but not very rigorous approach).\n",
    "Let's talk about frogs, and saving them.\n",
    "The greatest loss of vertebrate biodiversity we observed in the past 30 years\n",
    "is due to a chytrids fungus which is responsible for the extinction of over\n",
    "a hundred species of amphibians.\n",
    "![](Images/Frogs.jpg)\n",
    "\n",
    "Let's assume we have a sample space $S$ with all the possible outcomes of an experiment\n",
    "and we are interested in a subset of $S$, representing only some events.\n",
    "In our example we are interested in detecting which samples of frogs are infected\n",
    "or not by the fungus.\n",
    "For doing so, during our fieldwork, we take some samples and test whether they are\n",
    "infected or not.\n",
    "\n",
    "Let's consider $S$ to consist of all the samples collected in a particular area.\n",
    "We can split our $S$ in two events: the event \"samples with infection\"\n",
    "(designated as set $A$), and \"samples with no infection\" (complement of set $A$, or $A^c$).\n",
    "\n",
    "What is the probability that a randomly chosen sample is infected?\n",
    "It is the number of elements in $A$ divided by the number of elements of $S$.\n",
    "We can denote the number of elements of $A$ as $|A|$, called the cardinality of $A$.\n",
    "We can define the probability of $A$, $P(A)$, as\n",
    "\\begin{equation}\n",
    "    P(A) = \\frac{|A|}{|S|}\n",
    "\\end{equation}\n",
    "with $0 \\leq P(A) \\leq 1$.\n",
    "\n",
    "Let’s add another event.\n",
    "Assume that we use a molecular screening test which takes a biological sample\n",
    "(e.g. piece of skin) from a frog and tests for the presence of the fungus.\n",
    "Obviously, the test will be \"positive\" for some samples, and \"negative\" for\n",
    "some other samples.\n",
    "Let denote with event $B$ the collection of \"samples for which the test is positive\".\n",
    "What is the probability that the test will be \"positive\" for a randomly\n",
    "selected sample?\n",
    "It will be\n",
    "\\begin{equation}\n",
    "    P(B) = \\frac{|B|}{|S|}\n",
    "\\end{equation}\n",
    "with $0 \\leq P(B) \\leq 1$.\n",
    "\n",
    "We are now dealing with the entire sample space $S$ (all samples),\n",
    "the event $A$ (samples with infection), and the event $B$ (samples with a positive test).\n",
    "So far we have treated the two events separately, in isolation.\n",
    "What happens if we put them together?\n",
    "We can calculate the probability of both events occurring ($A \\cap B$)\n",
    "simultaneously.\n",
    "\\begin{equation}\n",
    "    P(A \\cap B) = \\frac{|A \\cap B|}{|S|}\n",
    "\\end{equation}\n",
    "with $0 \\leq P(A \\cap B) \\leq 1$.\n",
    "Note that sometimes $AB$ is used as shorthand notation for $A \\cap B$.\n",
    "The event $AB$ represents \"samples with infection and with a positive test\".\n",
    "Note that there is also the event $(B - AB)$ or \"samples without infection and\n",
    "with a positive test\", and the event $(A - AB)$ or \"samples with infection and with a\n",
    "negative test\".\n",
    "\n",
    "The question we want to answer now is \"given that the test is positive for a randomly\n",
    "selected sample, what is the probability that said sample is infected?\".\n",
    "In terms of a Venn diagram, this question translates to \"given that we are in region\n",
    "$B$, what is the probability that we are in region $A \\cap B$?\".\n",
    "This is equivalent of saying that \"if we make region $B$ our new Universe,\n",
    "what is the probability of $A$?\".\n",
    "The notation for the latter conditional probability is $P(A|B)$, called\n",
    "\"the probability of $A$ given $B$\".\n",
    "This probability is equal to\n",
    "\\begin{equation}\n",
    "    P(A|B) = \\frac{|A \\cap B|}{|B|} = \\frac{|A \\cap B|/|S|}{|B|/|S|} = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "If we ask the opposite question \"given that a randomly selected samples is infected\n",
    "(event $A$), what is the probability that the test is positive for that sample\n",
    "(event $A \\cap B$)?”, the conditional probability is\n",
    "\\begin{equation}\n",
    "    P(B|A) = \\frac{P(B \\cap A)}{P(A)}\n",
    "\\end{equation}\n",
    "If we put together these last two equations (by $P(B \\cap A)$) we obtain:\n",
    "\\begin{equation}\n",
    "  P(A)P(B|A) = P(B)P(A|B)\n",
    "\\end{equation}\n",
    "It follows that\n",
    "\\begin{equation}\n",
    " P(A|B) = \\frac{P(A)P(B|A)}{P(B)}\n",
    " \\end{equation}\n",
    "which is Bayes' theorem.\n",
    "\n",
    "### A Normal/Normal model\n",
    "\n",
    "We will now illustrate a simple Bayesian model to appreciate the interplay between likelihood and prior.\n",
    "Let's use again the example of infected frogs and imagine we want to\n",
    "assess the rate of infection in a particular geographical area.\n",
    "Let's assume that we monitored $20$ frogs in a given pond.\n",
    "We now want to make some inferences on the (total) number of\n",
    "infected frogs, $\\theta$, in the whole area.\n",
    "\n",
    "We consider the case where both the prior and the likelihood are Normal\n",
    "(Gaussian) distributions, that is \n",
    "\\begin{align}\n",
    " f(y|\\theta) &= N(y|\\theta, \\sigma^2)\\\\\n",
    " \\pi(\\theta) &= N(\\theta|\\mu, \\tau^2)\n",
    "\\end{align}\n",
    "Parameters $\\mu$ and $\\tau$ are known _hyperparameters_ while $\\theta$ is the unknown parameter.\n",
    "\n",
    "As we will learn later on more formally, the posterior distribution $p(\\theta|y)$\n",
    "is also a Normal distribution with parameters\n",
    "\\begin{equation}\n",
    "  p(\\theta|y) = N(\\theta|\\frac{\\sigma^2\\mu+\\tau^2y}{\\sigma^2+\\tau^2},\\frac{\\sigma^2\\tau^2}{\\sigma^2 + \\tau^2})\n",
    "\\end{equation}\n",
    "If we define $B$ as\n",
    "\\begin{equation}\n",
    "  B = \\frac{\\sigma^2}{\\sigma^2+\\tau^2}\n",
    "\\end{equation}\n",
    "then we can write\n",
    "\\begin{align}\n",
    " E(\\theta|y) &= B\\mu + (1-B)y \\\\\n",
    " Var(\\theta|y) &= (1-B)\\sigma^2 \\equiv B\\tau^2\n",
    "\\end{align}\n",
    "$B$ is called the _shrinking factor_ because it gives the proportion\n",
    "for how much the posterior mean is \"shrunk back\" from the classical\n",
    "frequentist estimate $y$ towards the prior mean $\\mu$.\n",
    "Note that $0 \\leq B \\geq 1$.\n",
    "\n",
    "The posterior mean is a weighted average of the prior mean $\\mu$ and\n",
    "the direct estimate $Y$.\n",
    "The weight on the prior mean $B$ depends on the relative variability\n",
    "of the prior distribution and the likelihood.\n",
    "If $\\sigma^2>>\\tau^2$ then $B \\approx 1$ and our prior knowledge is more\n",
    "precise than the data information.\n",
    "On the other hand, if $\\sigma^2<<\\tau^2$ then $B \\approx 0$ and our prior\n",
    "knowledge is imprecise and the final estimate will move very little towards\n",
    "the prior mean.\n",
    "\n",
    "We can appreciate this trade-off between data information and prior distribution\n",
    "with the following example.\n",
    "Assume that we have a single observation from one pond of $6$ infected frogs.\n",
    "In this case $y=6$ and we assume that our likelihood function (Normal)has $\\sigma=1$.\n",
    "Furthermore, let's assume that we expected $2$ infected frogs before observing\n",
    "the data.\n",
    "Therefore our prior distribution (Normal) has $\\mu=2$ and $\\tau=1$, for instance.\n",
    "More formally, we can write\n",
    "\\begin{align}\n",
    " f(y=6|\\theta) &= N(y=6|\\theta, 1) \\\\\n",
    " \\pi(\\theta) &= N(\\theta|2, 1)\n",
    "\\end{align}\n",
    "\n",
    "Let's use R to understand how this Normal/Normal model works in practice.\n",
    "Let's calculate and plot the prior, likelihood and posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior\n",
    "mu <- 2\n",
    "tau <- 1\n",
    "\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3) # prior\n",
    "\n",
    "# likelihood\n",
    "y <- 6\n",
    "sigma <- 1\n",
    "points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type=\"l\", lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3) # prior\n",
    "                                \n",
    "points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type=\"l\", lty=2) # likelihood\n",
    "\n",
    "# posterior\n",
    "B <- sigma^2/(sigma^2+tau^2)\n",
    "postMean <- B*mu + (1-B)*y\n",
    "postVar <- B*tau^2\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution is centred around 2 ($\\mu$), as expected.\n",
    "How about the likelihood function?\n",
    "It is centered around $6$ which is the only observation we have.\n",
    "The posterior distribution is centred exactly between the prior and the\n",
    "likelihood.\n",
    "Indeed, in this case $B=0.5$ and therefore prior and data are equally weighted.\n",
    "\n",
    "The _maximum a posteriori probability_ (MAP) estimate is $4$, as it\n",
    "is equal to the mode of the posterior distribution.\n",
    "You can easily retrieve the MAP for this example in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[which.max(dnorm(x=x, mean=postMean, sd=sqrt(postVar)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also observe that the posterior distribution is more skewed than\n",
    "the prior and the likelihood, despite these two distribution having the same variance.\n",
    "The posterior variance is smaller than the variance of either the prior\n",
    "or the likelihood.\n",
    "The _precision_, defined as the reciprocal of the variance, is the\n",
    "sum of the precisions in the prior and likelihood.\n",
    "The combined strength of prior and likelihood tends to increase the precision,\n",
    "or reduce the variance, in our inference of $\\theta$.\n",
    "In this example, the precision is $1+1=2$, hence the variance is $1/2$.\n",
    "Therefore the posterior roughly covers $4 \\pm 3(\\sqrt{1/2}) \\approx (1.88,6.12)$.\n",
    "\n",
    "What happens if we use a skewer (sharper) or wider prior?\n",
    "In other words, what is the shape of the posterior distribution if the\n",
    "variance of the prior is smaller (stronger belief) or larger (weaker belief)?\n",
    "\n",
    "__TASK__ Recalling the previous example, assume that the prior distribution has $\\mu=2$ and $\\tau=0.5$ (before we had $\\tau=1$). \n",
    "Calculate and plot prior and posterior distributions and evaluate the MAP in this case using R.\n",
    "Is the posterior mean closer or more distant from the prior mean? What is the shrinking factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewer prior\n",
    "mu <- 2\n",
    "tau <- ...\n",
    "\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)\n",
    "\n",
    "# likelihood\n",
    "...\n",
    "\n",
    "# posterior\n",
    "...\n",
    "\n",
    "# MAP and range\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK__ Recalling the previous example, assume that the prior distribution has $\\mu=2$ and $\\tau=2$ (before $\\tau \\leq 1$). Calculate and plot prior and posterior distributions and evaluate the MAP in this case using R.\n",
    "Is the posterior mean closer or more distant from the prior mean? What is the shrinking factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wider prior\n",
    "mu <- 2\n",
    "tau <- ...\n",
    "\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)\n",
    "\n",
    "# likelihood\n",
    "#...\n",
    "\n",
    "# posterior\n",
    "#...\n",
    "\n",
    "# MAP and range\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume now that we have more observations of infected frogs in multiple ponds.\n",
    "For instance, let's imagine that $\\vec{y}={6,5,5.5,6.5,6}$.\n",
    "Given a sample of $n$ independent observations, then\n",
    "\\begin{equation}\n",
    "    f(\\vec{y}|\\vec{\\theta}) = \\prod_{i=1}^n f(y_i|\\vec{\\theta}) \n",
    "\\end{equation}\n",
    "However, we can also use a transformation if we can find a statistic\n",
    "$S(\\vec{y})$ that is sufficient, meaning that\n",
    "$p(\\vec{\\theta}|\\vec{y})=p(\\vec{\\theta}|S(\\vec{y}))$.\n",
    "\n",
    "In this example, we can use the sufficient statistic $S(\\vec{y})=\\bar{y}$,\n",
    "where $\\bar{y}$ is the mean of $\\vec{y}$.\n",
    "The likelihood function has the form $f(\\bar{y}|\\theta)=N(\\theta, \\sigma^2/n)$\n",
    "and the posterior distributions is\n",
    "\\begin{equation}\n",
    "    p(\\theta|\\bar{y})=N(\\theta | \\frac{(\\sigma^2/n)\\mu + \n",
    "    \\tau^2\\bar{y}}{(\\sigma^2/n) + \\tau^2}, \\frac{(\\sigma^2/n)\\tau^2}{(\\sigma^2/n) + \\tau^2} )\n",
    "\\end{equation}\n",
    "Suppose we keep $\\mu=2$, $\\sigma=\\tau=1$ and set $\\bar{y}=5.8$ and $n=5$.\n",
    "Let's have a look at the resulting posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior (obviously it does not change)\n",
    "mu <- 2\n",
    "tau <- 1\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)\n",
    "\n",
    "# likelihood with more observations\n",
    "y <- c(6, 5, 5.5, 6.5, 6)\n",
    "n <- length(y)\n",
    "sigma <- 1\n",
    "points(x=x, y=dnorm(x=x, mean=mean(y), sd=sigma/n), type=\"l\", lty=2)\n",
    "\n",
    "# posterior with more observations\n",
    "postMean <- ( (sigma^2/n)*mu + tau^2*mean(y) ) / ( (sigma^2/n)*mu + tau^2 )\n",
    "postVar <- ( (sigma^2/n)*tau^2 ) / ( (sigma^2/n) + tau^2 )\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=3)\n",
    "\n",
    "# MAP with more observations\n",
    "map <-  x[which.max(dnorm(x=x, mean=postMean, sd=sqrt(postVar)))]\n",
    "cat(\"MAP:\", map, \"(\", map-3*sqrt(postVar),\",\",map+3*sqrt(postVar),\")\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily find that $\\theta_{MAP}=4.43$ with a range of $(3.21, 5.65)$.\n",
    "The MAP has been shifted towards the MLE as we have more data information.\n",
    "\n",
    "### Monte Carlo sampling\n",
    "\n",
    "To derive the posterior distribution we can also draw random samples from it,\n",
    "instead of directly calculating the posterior mean and variance (if a Normal distribution is considered, for instance).\n",
    "This procedure is often called _Monte Carlo sampling_, after the city famous for its casinos.\n",
    "\n",
    "In the previous example of the Normal/Normal model with multiple observations,\n",
    "we were able to calculate the posterior mean ($4.43$) and posterior variance ($0.17$).\n",
    "From these parameters, we were able to derive (and plot) the density function,\n",
    "the posterior probability itself.\n",
    "Alternatively, we can randomly sample directly from the posterior distribution.\n",
    "Let's see how we can do this in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte Carlo sampling\n",
    "par(mfrow=c(3,1))\n",
    "\n",
    "# posterior\n",
    "x <- seq(2,8,0.01)\n",
    "postMean <- 4.43\n",
    "postVar <- 0.16\n",
    "plot(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1,\n",
    "    ylab=\"Density\", xlab=expression(theta), main=expression(p(theta~\"|\"~y)),\n",
    "    ylim=c(0,1.2), xlim=c(2,8))\n",
    "\n",
    "# sampling\n",
    "y_sampled_1 <- rnorm(n=50, mean=postMean, sd=sqrt(postVar))\n",
    "hist(y_sampled_1, breaks=20, freq=F, lty=2, col=\"grey\", ylim=c(0,1.2), xlim=c(2,8),\n",
    "    sub=\"50 samples\", main=expression(p(theta~\"|\"~y)), xlab=expression(theta))\n",
    "    points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1)\n",
    "\n",
    "# more sampling\n",
    "y_sampled_2 <- rnorm(n=1e6, mean=postMean, sd=sqrt(postVar))\n",
    "hist(y_sampled_2, breaks=20, freq=F, lty=2, col=\"grey\", ylim=c(0,1.2), xlim=c(2,8),\n",
    "    sub=\"1e6 samples\", main=expression(p(theta~\"|\"~y)), xlab=expression(theta))\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1, ylab=\"Density\",\n",
    "    xlab=expression(theta), main=expression(p(theta~\"|\"~y)), sub=\"1e6 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more sampling we do, the closer our sampled distribution will be to the \"true\" posterior distribution.\n",
    "With 50 samples the empirical posterior mean is $4.444$ while with $1e6$ samples we have an\n",
    "empirical posterior mean of $4.429$ which is very close to our direct estimate of $4.43$.\n",
    "With 50 samples the empirical posterior variance is $0.105$ while with $1e6$ samples we have an\n",
    "empirical posterior variance of $0.159$ which is very close to our direct estimate of $0.16$.\n",
    "\n",
    "You can eaisly see how, in this simple Normal/Normal case, Monte Carlo methods are not\n",
    "strictly necessary since the integral in the denominator of Bayes' theorem can be evaluated\n",
    "in closed forms (as we will see later on).\n",
    "In these such cases, it is preferable prefer to derive a smooth curve rather an a histogram\n",
    "of sampled values, and have the corresponding exact values for the posterior parameters.\n",
    "However there are cases where, given the choice for the likelihood and prior functions,\n",
    "this integral cannot be evaluated.\n",
    "Therefore, in these cases, Monte Carlo methods are to be preferred for estimating, or rather\n",
    "approximating the posterior distribution.\n",
    "As any sample can be drawn from any posterior regardless of how many parameters $\\theta$\n",
    "you may have, we have the ability to work on problems with (theoretically) unlimited complexity,\n",
    "at the price of not obtaining an exact form for the posterior and performing a large number of samplings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Practical: reconstructing genomes from sequencing data\n",
    "\n",
    "We want to use a Bayesian approach to reconstruct genomes from data produced from\n",
    "high-throughput sequencing machines.\n",
    "First, load the R functions needed with `source(\"functions.R\")`.\n",
    "\n",
    "Among these functions, we provide one that calculates the likelihood of a certain sequence of bases\n",
    "for diploid individuals.\n",
    "This function is called _calcGenoLikes_ and takes 5 paramaters in input:\n",
    "* the sequence itself (collection of bases)\n",
    "* the first allele of the genotype\n",
    "* the second allele of the genotype\n",
    "* the sequencing error rate\n",
    "* a boolean indicating whether the results should be returned in logarithmic scale (TRUE) or not (FALSE)\n",
    "\n",
    "For instance, assuming that your sequence is `AAGAGGA`, your alleles are `A` and `G` (meaning that you want to calculate the likelihood for genotypes `{AA,AG,GG}`, and your sequencing error rate is 0.05, then the likelihood (not in logarithms) for each genotype is given by \n",
    "```calcGenoLikes(\"AAGAGGA\", \"A\", \"G\", 0.05, FALSE)```\n",
    "\n",
    "Complete all the following tasks using R when necessary.\n",
    "The key point of these exercises is to not recalculate quantities that you have already computed.\n",
    "The aim is that you should be able to understand whether the likelihood or the prior is the same (or not)\n",
    "between different scenarios.\n",
    "\n",
    "___A)___ Using Bayes' theorem, write the formula for the posterior probability of genotype G being\n",
    "AA given the sequencing data D.\n",
    "Write the explicit denominator assuming that your alleles are A and G and all possible genotypes are only AA, AG, GG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___B)___ Assuming that your data is `AAAG`, your alleles are A and G, and the sequencing error rate is 0.01,\n",
    "calculate genotype posterior probability using a uniform prior, e.g. $P(G=AA) = P(G=AG) = P(G=GG) = ?$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___C)___ With the same assumptions as in point B, calculate genotype posterior probabilities using\n",
    "prior probabilitties based on [Hardy Weinberg Equilibrium](https://en.wikipedia.org/wiki/Hardy–Weinberg_principle) with a frequency of G of 0.1. Do you need to calculate a new likelihood or is it the same one as in point B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___D)___  With the same assumptions as in point C, calculate genotype posterior probabilities using a prior based on Hardy Weinberg Equilibrium with a frequency of G of 0.1 and an [inbreeding coefficient](https://en.wikipedia.org/wiki/Inbreeding) of 0.2.\n",
    "In this case, we need to modify our previous priors.\n",
    "Specifically, if $f$ is the frequency of allele A and $I$ is the inbreeding coefficient,\n",
    "then the prior probabilities for all genotypes are:\n",
    "* $p(AA)=f^2 + I \\times f \\times (1-f)$\n",
    "* $p(AT)=2 \\times f \\times (1-f) \\times (1-I)$\n",
    "* $p(TT)=(1-f)^2 + I \\times f \\times (1-f)$\n",
    "Do you need to calculate a new likelihood or is it the same one as in points B and C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___E)___ With the same priors used in point D but with a sequencing error rate of 0.05, calculate the genotype posterior probabilities. Do you need to calculate a new likelihood or is it the same one as in point D?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___F)___ Plot all previous results (e.g. use a barplot with the 3 posterior probabilities for each scenario B-E)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___G___) Assuming that our collection of sequenced bases is `AAAGAGAAAAAAAGGGGGAAAGGA`, calculate the genotype posterior probabilities using the same priors as in point C. What happens if we have more data? What is the confidence in our genotype inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___H)___ What happens if we have a lot  of data? Assume that your sequenced bases are\n",
    "`bases <- paste(c(rep(\"A\",1e3),rep(\"G\",1e3)), sep=\"\", collapse=\"\")`.\n",
    "What happens here?\n",
    "It is convenient to use numbers in log-scale: $log(a \\times b / c) = log(a) + log(b) -log(c)$.\n",
    "You can do that by selecting TRUE as the last parameter in the _calcGenoLikes_.\n",
    "What is the effect of the prior here?\n",
    "Remember that if you want to calculate proper probabilities (in log) you have to [approximate](https://en.wikipedia.org/wiki/List_of_logarithmic_identities)\n",
    "the sum of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "## Prior distributions\n",
    "\n",
    "One of the main feature of Bayesian statistics is that we assign probability distributions not only to\n",
    "data variables ${y}$ but also to parameters ${\\theta}$.\n",
    "In other words, we quantify whatever feelings or believes we have about ${\\theta}$ before\n",
    "observing ${y}$.\n",
    "Using Bayes' theorem, we obtain a posterior distribution of\n",
    "${\\theta}$, a blend of the information between the data and the prior.\n",
    "\n",
    "How can we decide which prior distribution is more appropriate in our study?\n",
    "As you can imagine this can be an arduous task and hampered the use of Bayesian statistics in the past.\n",
    "In general, prior distributions are derived from past information or personal opinions from experts.\n",
    "It is typical to restrict prior probabilities to be distributed as familiar distribution families.\n",
    "Alternatively, one can limit the prior distribution to bear little information.\n",
    "We now review some possibilities to assign prior distributions.\n",
    "\n",
    "### Elicited priors\n",
    "\n",
    "The simplest approach to specify $\\pi(\\theta)$ is to define the collection of $\\theta$ which are possible.\n",
    "Then one can assign some probability to each one of these cases and make sure that they sum to $1$.\n",
    "If $\\theta$ is discrete, this looks like a natural approach.\n",
    "\n",
    "Let's understand this use of elicited priors with an example.\n",
    "Imagine that your prior distribution describes the number of kits a mother\n",
    "rabbit will have in the next litter.\n",
    "![](Images/Rabbits.jpeg)\n",
    "Perhaps, you want to make some inference on the biological mechanisms for the number of kits.\n",
    "In this case you may have a likelihood function relating some observations $\\vec{y}$\n",
    "(e.g. genetic or environmental markers) to the number of kits $\\theta$.\n",
    "\n",
    "$\\theta$ is clearly discrete and you may have some past information on its distribution.\n",
    "For instance, from the [literature](www.reference.com/pets-animals), we know that \"Rabbits can have \n",
    "anywhere from one to 14 babies, also called kits, in one litter. An average litter size is 6. \n",
    "Hereditary and environmental factors play a role in the number of kits born in a litter.\"\n",
    "Therefore, as a first try, you can assign\n",
    "\\begin{equation}\n",
    "    \\pi(\\theta=0)=\\pi(\\theta>14)=0\n",
    "\\end{equation}\n",
    "Secondly, you can impose that it is more probable that this mother will have $6$ kits,\n",
    "as this is the average litter size based on past information, as\n",
    "\\begin{equation}\n",
    "    \\pi(\\theta=2)<\\pi(\\theta=6)>\\pi(\\theta=10)\n",
    "\\end{equation}\n",
    "Finally, you must ensure that\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{14} \\pi(\\theta=i) = 1\n",
    "\\end{equation}\n",
    "\n",
    "On the other hand, it $\\theta$ is continuous, a simple solution would be to discretise\n",
    "our prior distribution, by assigning masses to intervals.\n",
    "In other words, you create a histogram prior for $\\theta$.\n",
    "Imagine that your prior distribution specifies the recorded temperature in hot springs\n",
    "at Lassen Volcanic National Park.\n",
    "Specifically, you are interested in relating the temperature of different pools at Bumpass\n",
    "Hell with the occurrence of certain extremophile micro-organisms, capable of surviving in extremely hot environments.\n",
    "![](Images/BumpassHell.jpeg)\n",
    "\n",
    "For instance, you may want to predict the temperature of the pool from the observations\n",
    "of such micro-organisms.\n",
    "The latter distribution is the likelihood function.\n",
    "However, you also want to assign a prior distribution for the pool temperature, $\\theta$.\n",
    "Clearly $\\theta$ is continuous and, from past observations, we know that it has a range\n",
    "of $(80,110)$ with an average of $88$, in Celsius degrees.\n",
    "A simple solution would be to derive a prior histogram of $\\theta$, as\n",
    "\\begin{equation}\n",
    "    \\pi(80 \\geq \\theta < 85) < \\pi(85 \\geq \\theta < 90) > \\pi(90 \\geq \\theta < 95)\n",
    "\\end{equation}\n",
    "Again, you have to make sure that all these probabilities sum to $1$.\n",
    "Furthermore, it is important that the histogram is sufficiently wide, as the posterior\n",
    "will have support only for values that are included in the prior.\n",
    "\n",
    "Alternatively, we may assume that the prior distribution for $\\theta$ belongs to a\n",
    "parametric distributional family $\\pi(\\theta|\\nu)$.\n",
    "Here we choose $\\nu$ so that $\\pi(\\theta|\\nu)$ closely matches our elicited beliefs.\n",
    "This approach has several advantages, including:\n",
    "* it reduces the effort to the elicitee (you don't have to decide a probability for each value $\\theta$ can have);\n",
    "* it overcomes the finite support problem (as in the case of the histogram);\n",
    "* it may lead to simplifications in the computation of the posterior (as we will see later on).\n",
    "A limitation of this approach is that it would be impossible to find a distribution that perfectly matches the elicitee's beliefs.\n",
    "\n",
    "For instance, the prior for temperatures could be Normally distributed as $N(\\mu,\\sigma^2)$ bounded at $(80,110)$, that is\n",
    "\n",
    "\\begin{align}\n",
    "        \\pi(\\theta) &= 0 & \\text{for } \\theta < 80 \\text{ or } \\theta > 110 \\\\\n",
    "        \\pi(\\theta) &= N(\\mu,\\sigma^2) & \\text{for } 80 \\leq \\theta \\leq 110\n",
    "\\end{align}\n",
    "\n",
    "with $\\mu=88$ and $\\sigma^2=10$.\n",
    "We can plot this distribution in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## elicited prior\n",
    "mu <- 88\n",
    "sigma2 <- 10\n",
    "x <- seq(80,110,0.1)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=sqrt(sigma2)), type=\"l\", lty=1, ylab=\"Density\",\n",
    "    xlab=expression(theta), main=expression(pi(theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this distribution is not defined outside the interval $(80,110)$.\n",
    "Consequentely, the posterior distribution will not have mass outside this interval.\n",
    "Overconfidence may result into failing to condition on events outside the range\n",
    "of personal experience or previous observations.\n",
    "For instance, the fact that a temperature lower than $80$ has never been observed may\n",
    "be better represented by setting a small (but greater than 0) probability of occurring.\n",
    "\n",
    "As a rule of thumb, for elicited priors, it is recommended to focus on quantiles\n",
    "close to the middle of the distribution (e.g. the $50^{th}$, $25^{th}$ and $75^{th}$)\n",
    "rather than extreme quantiles (e.g. the $95^{th}$ and $5^{th}$).\n",
    "You should also assess the symmetry of your prior.\n",
    "Elicited priors can be updated and reassessed as new information is available.\n",
    "They are very useful for experimental design where some ideas on the nature of the\n",
    "studied system is given in input.\n",
    "\n",
    "### Conjugate priors\n",
    "\n",
    "When choosing a prior distribution $\\pi(\\theta|\\nu)$ some family distributions will make\n",
    "the calculation of posterior distributions more convenient than others will do.\n",
    "It is possible to select a member of that family that is _conjugate_ with the\n",
    "likelihood $f(y|\\theta)$, so that the posterior distribution $p(\\theta|y)$\n",
    "belongs to the same distributional family as the prior.\n",
    "\n",
    "Let's illustrate this point with an example.\n",
    "Suppose we are interested in modelling the arrival of herds of elephants to a specific\n",
    "water pond in the savannah in a day during the migratory season.\n",
    "We may be interested in this estimate for tracking migratory routes or assessing population sizes.\n",
    "$Y$ is the count of distinct elephant herds or groups (not the number of elephants itself!)\n",
    "arriving at the pool in a day during the migration season (not during the whole year!).\n",
    "![](Images/Elephants.jpeg)\n",
    "\n",
    "A Poisson distribution has a natural interpretation to model arrival rates\n",
    "for discrete variables.\n",
    "Indeed, the Poisson distribution is a discrete probability distribution that gives\n",
    "the probability of a given number of events occurring in a fixed interval of time\n",
    "(or space) when such events occur independently and with a known average rate.\n",
    "\n",
    "The Poisson distribution is an appropriate model under certain assumptions:\n",
    "1. $Y$ is the number of times an event occurs in an interval and it can take values any positive integer values including 0;\n",
    "2. the occurrence of one event does not affect the probability that a second event will occur (i.e. events occur independently);\n",
    "3. the rate at which events occur is constant (it cannot be higher in some intervals and lower in other intervals);\n",
    "4. two events cannot occur at exactly the same instant;\n",
    "5. the probability of an event in an interval is proportional to the length of the interval.\n",
    "\n",
    "Condition 1 is clearly met in our case.\n",
    "Conditions 2 and 4 assumes that different herds do not follow each other (perhaps\n",
    "by taking different routes).\n",
    "For the sake of illustrating this distribution, we will assume this to be true.\n",
    "You can see that if $Y$ were the number of elephants (not the herds) then condition 2 is\n",
    "not met as elephants tend to migrate in group.\n",
    "Condition 3 is met when we focus our analysis on the annual period where we expect\n",
    "to see herds, not during the whole year.\n",
    "Condition 5 is easily met, as the number of herds arriving in a week is likely to be\n",
    "higher than the number in a day.\n",
    "If we assume that all these conditions are true, then $Y$, the number of elepehant herds\n",
    "arriving at a pool, is a Poisson random variable.\n",
    "\n",
    "The event under study ($y$, number of herds) can occur $0, 1, 2, ...$ times in the\n",
    "interval (a day).\n",
    "The average number of events (herds arriving) in an interval (one day) is designated\n",
    "our parameter $\\theta$.\n",
    "Note that the parameter of the Poisson distribution is typically written as $\\lambda$.\n",
    "In our example, $\\theta$ is the event rate, or the rate parameter.\n",
    "As such, the probability of observing $y$ events in an interval is given by\n",
    "\\begin{equation}\n",
    "    f(y|\\theta) = \\frac{e^{-\\theta}\\theta^y}{y!} \\text{, } y \\in \\{0, 1, 2, ...\\} \n",
    "        \\text{, } \\theta>0\n",
    "\\end{equation}\n",
    "\n",
    "This is our likelihood distribution and, once we know $\\theta$, then the whole\n",
    "distribution is defined.\n",
    "As you can see, $\\theta$ has to be positive (not necessarily an integer) and $y$\n",
    "is a positive integer.\n",
    "Note that the equation above is a probability mass function (pmf),\n",
    "as it is defined only for discrete values of $y$.\n",
    "\n",
    "For instance, assuming that, based on previous observations, the rate $\\theta$ is set to $4$\n",
    "(4 herds per day during migration season), then:\n",
    "\\begin{align}\n",
    "    P(y=0) &= \\frac{e^{-4}4^0}{0!} = e^{-4} = 0.0183\\\\\n",
    "    P(y=1) &= \\frac{e^{-4}4^1}{1!} = ... = 0.0733\\\\\n",
    "    P(y=2) &= \\frac{e^{-4}4^2}{2!} = ... = 0.147\n",
    "\\end{align}\n",
    "We can plot this likelihood distribution for $\\theta=4$ using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Poisson distribution\n",
    "theta <- 4\n",
    "y <- seq(0, 20, 1)\n",
    "plot(x=y, dpois(x=y, lambda=theta), type=\"p\", lty=1, xlab=expression(y),\n",
    "    main=expression(theta~\"=\"~theta), ylab=expression(p(Y~\"=\"~y~\"|\"~theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the highest mass is towards 4 and above 12 the probability is very close to 0.\n",
    "You may recall that a Poisson distribution has expected value and variance equal\n",
    "to the rate parameter.\n",
    "Note that we have some non-zero probability of observing 0 events.\n",
    "\n",
    "If we want to perform a Bayesian analysis, we need to define a prior distribution\n",
    "for $\\theta$ having support for positive values.\n",
    "In other words, we assume that we don't know the exact value of $\\theta$, but we can\n",
    "make some assumptions on its probability distribution.\n",
    "A reasonable choice (as we will appreciate later) is given by the Gamma distribution\n",
    "\\begin{equation}\n",
    "    \\pi(\\theta) = \\frac{\\theta^{\\alpha-1}e^{-\\theta/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \n",
    "    \\text{, } \\theta>0 \\text{, } \\alpha>0 \\text{, } \\beta>0\n",
    "\\end{equation}\n",
    "\n",
    "The Gamma distribution will be our prior distribution.\n",
    "We can denote this as $\\theta \\sim G(\\alpha,\\beta)$.\n",
    "As you can evince, the Gamma distribution is a two-parameter family of continuous\n",
    "probability distributions.\n",
    "Please note that the common exponential distribution and chi-squared distribution are\n",
    "special cases of the Gamma distribution.\n",
    "We have also suppressed the dependency of $\\pi$ to hyperparameters $\\nu=(\\alpha,\\beta)$\n",
    "since we assume them to be known.\n",
    "$\\alpha$ is known as the shape parameter while $\\beta$ is the rate parameter.\n",
    "The expected value and variance for the Gamma distribution are\n",
    "\\begin{align}\n",
    "    E(G(\\alpha,\\beta)) &= \\alpha\\beta\\\\\n",
    "    Var(G(\\alpha,\\beta)) &= \\alpha\\beta^2\n",
    "\\end{align}\n",
    "Let's have a look at the shape of the Gamma distribution for different values of\n",
    "its parameters using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gamma distribution\n",
    "alpha <- c(0.5,2,10)\n",
    "beta <- c(0.5,2)\n",
    "thetas <- seq(0, 20, 0.1)\n",
    "\n",
    "mycolors <- topo.colors(6)\n",
    "plot(x=thetas, dgamma(x=thetas, shape=alpha[1], scale=beta[1]), type=\"l\",\n",
    "    lty=1, xlab=expression(theta), main=\"Gamma\", ylab=expression(pi(theta)),\n",
    "    ylim=c(0,1.0), col=mycolors[1], lwd=2)\n",
    "\n",
    "index <- 0\n",
    "for (i in alpha) {\n",
    "    for (j in beta) {\n",
    "        index <- index+1\n",
    "        points(x=thetas, dgamma(x=thetas, shape=i, scale=j),col=mycolors[index], ty=\"l\", lwd=2)\n",
    "    }\n",
    "}\n",
    "\n",
    "names <- cbind(rep(alpha,each=2),rep(beta))\n",
    "\n",
    "legend(x=\"topright\", legend=apply(FUN=paste, MAR=1, X=names, sep=\"\", collapse=\", \"), col=mycolors, lty=1, lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from this plot, the Gamma distribution is very flexible\n",
    "and it can have one tail ($\\alpha \\leq 1$) or two tails ($\\alpha > 1$).\n",
    "For very large values of $\\alpha$ the Gamma distribution resembles a Normal distribution.\n",
    "The $\\beta$ parameter shrinks or stretches the distribution relative to 0 but\n",
    "it doesn't change its shape.\n",
    "\n",
    "Using the Bayes' theorem, we can now obtain the posterior probability\n",
    "\\begin{align}\n",
    "    P(\\theta|y) &\\approx f(y|\\theta)/\\pi(\\theta)\\\\\n",
    "                &\\approx (e^{-\\theta}\\theta^y)(\\theta^{\\alpha-1}e^{-\\theta/\\beta})\\\\\n",
    "                &= \\theta^{y+\\alpha-1}e^{-\\theta(1+1/\\beta)}\n",
    "\\end{align}\n",
    "Since we are only interested in a normalised function of $\\theta$, we drop all functions\n",
    "that do not depend on $\\theta$.\n",
    "You may realise that this posterior distribution is actually a Gamma distribution\n",
    "$G(\\alpha',\\beta')$ with $\\alpha'=y+\\alpha$ and $\\beta'=(1+1/\\beta)^{-1}$.\n",
    "We were able to do this operation because the Gamma distribution (our prior) is the conjugate\n",
    "family for the Poisson distribution (our likelihood).\n",
    "\n",
    "To finalise our example of elephant herds, suppose that, before looking at the actual data,\n",
    "we have some intuition that we expect to see 3 herds per day.\n",
    "This could be derived from observations in previous years.\n",
    "Let's also assume that we are not very confident and we assign a large variance to it.\n",
    "For instance, our prior distribution is $G(3,1)$ which points to an expected value\n",
    "and variance of 3.\n",
    "We then observed 4 herds ($y=4$).\n",
    "Therefore, our posterior distribution will be a gamma distribution $G(3+4,1/(1+1/1)$.\n",
    "\n",
    "Let's plot these distributions in R. As an illustration, we also plot the posterior distribution obtained\n",
    "by Monte Carlo sampling.\n",
    "Conjugate priors allow for posterior distributions to emerge without numerical integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gamma posterior\n",
    "alpha <- 3\n",
    "beta <- 1\n",
    "theta <- seq(0, 20, 0.1)\n",
    "\n",
    "prior <- dgamma(x=theta, shape=alpha, scale=beta)\n",
    "y <- 4\n",
    "posterior <- dgamma(x=theta, shape=y+alpha, scale=1/(1+1/beta))\n",
    "\n",
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Density\", type=\"l\")\n",
    "lines(theta, prior, lty=3)\n",
    "postdraw <- rgamma(n=1e5, shape=y+alpha, scale=1/(1+1/beta))\n",
    "histdraw <- hist(postdraw, breaks=20, plot=F)\n",
    "lines(histdraw, lty=3, col=\"grey\", freq=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical modelling\n",
    "\n",
    "There are many other ways to build a prior distribution.\n",
    "As we have seen so far, a posterior distribution is typically obtained with two stages,\n",
    "one for $f({y},{\\theta})$, the likelihood of the data, and\n",
    "$\\pi({\\theta}, {\\nu})$, the prior distribution of ${\\theta}$ given a vector\n",
    "of _hyperparameters_ ${\\nu}$.\n",
    "Note that we drop the notation of all these parameters being vectors for simplicity.\n",
    "\n",
    "Let's suppose that we are uncertain about the values for ${\\nu}$.\n",
    "In this case, we need an additional stage, a _hyperprior_, defining the density\n",
    "distribution of hyperparameters.\n",
    "If we denote this distribution as $h({\\nu})$, then the posterior distribution is\n",
    "\\begin{equation}\n",
    "P({\\theta}|{y}) = \\frac{ \\int f({y}|{\\theta})\\pi({\\theta}|{\\nu})\n",
    "               h({\\nu})d{\\nu} }{ \\int \\int f({y}|{\\theta})\\pi({\\theta}|\n",
    "              {\\nu})h({\\nu})d{\\nu}d{\\theta} } \n",
    "\\end{equation}\n",
    "\n",
    "Another possibility is to replace ${\\nu}$ with an estimate $\\hat{{\\nu}}$\n",
    "obtained by maximising the marginal distribution $m({y}|{\\nu})$.\n",
    "In this way, our inferences are made on the _estimated posterior_\n",
    "$P({\\theta}|{y},\\hat{{\\nu}})$, by inserting $\\hat{{\\nu}})$ in the Bayes' theorem equation.\n",
    "This approach is called _Empirical Bayesian_ analysis as we are using the data\n",
    "to estimate the hyperparameter.\n",
    "\n",
    "The empirical estimation of the prior seems a violation of Bayesian principles.\n",
    "Indeed, the update of the prior based on the data would use the data twice, both for the\n",
    "likelihood and the prior.\n",
    "Inferences from such modelling tend to be \"overconfident\" and methods that ignore\n",
    "this fact are called _naive Empirical Bayesian_ approaches.\n",
    "\n",
    "Furthermore, we can again think of ${\\nu}$ depending on a collection of\n",
    "unknown parameters ${\\lambda}$, with $h({\\nu}|{\\lambda})$ and a third-stage prior $g({\\lambda})$.\n",
    "This procedure of specifying a model over several layers is called _hierarchical modelling_.\n",
    "This framework is very much used in graphical modelling (e.g. Bayesian networks).\n",
    "As we add extra layers and levels of randomness, subtle changes at the top levels\n",
    "(hyperpriors) will not have a strong effect at the bottom level (the data).\n",
    "\n",
    "### Noninformative priors\n",
    "\n",
    "It is often the case that no reliable prior information on ${\\theta}$ is available.\n",
    "Can we employ a Bayesian approach under such circumstances?\n",
    "It is still appropriate if we find a distribution $\\pi({\\theta})$ that contains\n",
    "\"no information\" about ${\\theta}$, in the sense that it does not favour\n",
    "one value over another.\n",
    "We refer to such a distribution as a _noninformative prior_ for ${\\theta}$.\n",
    "All the information in the posterior will arise from the data.\n",
    "\n",
    "Suppose that the parameter space is discrete and finite, that is, $\\vec{\\Theta}=\\{\\theta_1, \\theta_2, ..., \\theta_n\\}$.\n",
    "What is a possible noninformative prior?\n",
    "One possibility is\n",
    "\\begin{equation}\n",
    "    p(\\theta_i) = \\frac{1}{n} \\text{, } i=1, 2, ...,n \n",
    "\\end{equation}\n",
    "This prior distribution does not favour any value and therefore it is\n",
    "noninformative about $\\theta$.\n",
    "Furthermore, note that even in this case the prior distribution must be a\n",
    "proper (legitimate) probability distribution, meaning that\n",
    "\\begin{equation}\n",
    "    \\sum_1^n \\frac{1}{n}=1 \n",
    "\\end{equation}\n",
    "\n",
    "If $\\vec{\\Theta}$ is continuous and bounded, as $\\vec{\\Theta}=[a,b]$ with $-\\infty<a<b<+\\infty$,\n",
    "a uniform prior in the form\n",
    "\\begin{equation}\n",
    "    P(\\theta) = \\frac{1}{b-a} \\text{, } a<\\theta<b\n",
    "\\end{equation}\n",
    "is a noninformative prior distribution, although this assertion is less clear to be true\n",
    "than in the discrete case.\n",
    "\n",
    "How about the case of $\\vec{\\Theta}$ being continuous and unbounded, so that\n",
    "$\\vec{\\Theta}=(-\\infty,+\\infty)$?\n",
    "A noninformative prior could be set as\n",
    "\\begin{equation}\n",
    "    P(\\theta) = c \\text{, any } c>0 \n",
    "\\end{equation}\n",
    "However, this distribution is clearly _improper_ as\n",
    "\\begin{equation}\n",
    "    \\int_{-\\infty}^{+\\infty} p(\\theta) d\\theta = +\\infty \n",
    "\\end{equation}\n",
    "This may suggest that in this case a Bayesian approach cannot be used.\n",
    "However a Bayesian inference is still possible if the integral of of the likelihood\n",
    "$f({y}|\\theta)$ with respect to $\\theta$ equals some finite value $K$, since\n",
    "\\begin{equation}\n",
    "    \\int \\frac{f({y}|\\theta) \\cdot c}{\\int f({y}|\\theta) \\cdot c d\\theta} d\\theta = 1\n",
    "\\end{equation}\n",
    "\n",
    "Let's go back to our example on herds of elephants.\n",
    "Suppose that we don't have information on past rates of arrivals and therefore\n",
    "we cannot (or don't want to) use the Gamma distribution as prior distribution as previously done.\n",
    "Instead, we use a uniform prior $U$ for the mean arrival $\\theta$, our parameter.\n",
    "What is the interval of $\\theta$?\n",
    "Theoretically, it should be $[0,+\\infty)$.\n",
    "However let's limit it to a large number and define a uniform prior distribution as $U(0,100)$.\n",
    "Let's plot this distribution in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uniform distribution\n",
    "theta <- seq(0, 100, 0.1)\n",
    "prior <- dunif(x=theta, min=0, max=100)\n",
    "plot(x=theta, y=prior, xlab=expression(theta), ylab=\"Density\", type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This choice rules out scenarios that are impossible in real life.\n",
    "This means that the posterior will be truncated at 0 and 100.\n",
    "As we lack a conjugate model, we can sample from the posterior to obtain its distribution.\n",
    "We will see later how we can do this using computational methods.\n",
    "Noninformative priors are related to the notion of _reference_ priors.\n",
    "These are not necessarily noninformative but a convenient, default choice for prior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Once we have specified the prior, we can use Bayes' theorem to obtain the posterior\n",
    "distribution of model parameters.\n",
    "However, the density (or cumulative) function can be difficult to interpret.\n",
    "Therefore we want to summarise the information enclosed in these distributions.\n",
    "Here we discuss how to develop Bayesian techniques for point estimation, interval estimation, and hypothesis testing.\n",
    "\n",
    "### Point estimation\n",
    "\n",
    "Let's first consider the univariate case.\n",
    "We want to select a summary feature of $p(\\theta|{y})$ to obtain a point estimate \n",
    "$\\hat{\\theta}({y})$, which could be either its mean, median, or mode.\n",
    "These features may behave very differently depending on the distribution, especially\n",
    "when it is asymmetric and one-tailed.\n",
    "Recalling the example of elephant herds, assuming $y=1$ and that our prior is $G(0.5,1)$,\n",
    "let's calculate and plot the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gamma posterior asymmetric, one-tailed prior\n",
    "alpha <- 0.5\n",
    "beta <- 1\n",
    "theta <- seq(0, 20, 0.1)\n",
    "\n",
    "prior <- dgamma(x=theta, shape=alpha, scale=beta)\n",
    "y <- 1\n",
    "\n",
    "posterior <- dgamma(x=theta, shape=y+alpha, scale=1/(1+1/beta))\n",
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Density\", type=\"l\")\n",
    "lines(theta, prior, lty=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean, mode and median of the resulting posterior distribution?\n",
    "Generally speaking, the mode is the easiest to calculate.\n",
    "Since no normalisation is required, we can work directly with the numerator.\n",
    "Note that if the prior distribution is flat, then the _posterior mode_ will be equal to the maximum likelihood estimate of $\\theta$.\n",
    "In this case, it is called the _generalised maximum likelihood_ estimate of $\\theta$.\n",
    "\n",
    "If the posterior distribution is symmetric, then the mean and the median are equivalent.\n",
    "For symmetric unimodal distributions, all these three features are equivalent.\n",
    "For asymmetric distributions, the median is often the best choice as it is less affected\n",
    "by outliers and it is an intermediate to the mode and the mean.\n",
    "\n",
    "If we want to obtain a measure of accuracy of a point estimate $\\hat{\\theta}({y})$,\n",
    "we can calculate the _posterior variance_\n",
    "\\begin{equation}\n",
    "    \\mathrm{Var}_{\\theta|{y}}(\\theta) = \\mathrm{E}_{\\theta|{y}}[\\theta-\\mathrm{E}_{\\theta({y})}]^2 \n",
    "\\end{equation}\n",
    "The posterior mean minimises the posterior variance in respect to $\\hat{\\theta}({y})$.\n",
    "\n",
    "In the multivariate case, we can obtain the posterior mode as\n",
    "$\\hat{\\vec{\\theta}}({y})=(\\hat{\\theta_1},\\hat{\\theta_2},...,\\hat{\\theta_k})$.\n",
    "If the mode exists, maximisation methods (e.g. grid search, golden section search,\n",
    "Newton-type methods, ...) are typically employed to locate the maximum.\n",
    "We can calculate the posterior mean which, again, minimises the _posterior covariance matrix_ with respect to $\\hat{\\vec{\\theta}}({y})$.\n",
    "\n",
    "### Credible intervals\n",
    "\n",
    "The Bayesian analogue of a frequentist confidence interval is called a _credible set_.\n",
    "A $100 \\times (1-\\alpha)$ credible set for ${\\theta}$ is a subset $C$ of\n",
    "${\\Theta}$ such that\n",
    "\\begin{equation}\n",
    "    1-\\alpha \\leq P(C|{y}) = \\int_C p({\\theta}|{y})d{\\theta}\n",
    "\\end{equation}\n",
    "in the continuous case.\n",
    "In the discrete case the integral is replaced by a summation.\n",
    "\n",
    "This definition can express a likelihood of $\\theta$ falling in $C$ as \"the probability that $\\theta$ lies in $C$ given the observed data $y$ is at least $(1-\\alpha)$\".\n",
    "Unlike the frequentist case, the credible set provides an actual probability statement,\n",
    "based on both the observed data and prior opinion.\n",
    "In discrete settings it may not be possible to find the exact coverage probability $(1-\\alpha)$.\n",
    "In continuous settings we can calculate the _highest posterior density_, or HPD, credible set, defined as\n",
    "\\begin{equation}\n",
    "    C = \\{ \\theta \\in \\Theta : p(\\theta|y) \\geq k(\\alpha) \\}\n",
    "\\end{equation}\n",
    "where $k(\\alpha)$ is the largest constant satisfying $P(C|y)\\geq(1-\\alpha)$.\n",
    "\n",
    "Let's recall the example of elephants herds and assume that the posterior distribution\n",
    "$p(\\theta|{y}) \\sim G(2,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interval estimation\n",
    "theta <- seq(0, 10, 0.05)\n",
    "alpha <- 2\n",
    "beta <- 1\n",
    "\n",
    "posterior <- dgamma(x=theta, shape=alpha, scale=beta)\n",
    "\n",
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Posterior density\", type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to define a credible interval.\n",
    "In the HPD, we choose the narrowest interval which, for a unimodal distribution, involves\n",
    "choosing those values of highest probability density.\n",
    "This interval will include the mode for a unimodal distribution.\n",
    "For instance, drawing a line at $k(\\alpha)=0.1$ results in a $87\\%$ HPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Posterior density\", type=\"l\")\n",
    "abline(h=0.1, lty=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can calculate the interval of values of $\\theta$ included in the $87\\%$ HPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"coda\") # uncomment if you don't have \"coda\" R package already installed\n",
    "library(coda)\n",
    "x <- rgamma(n=1e5, shape=alpha, scale=beta)\n",
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Posterior density\", type=\"l\")\n",
    "hpd <- HPDinterval(as.mcmc(x), prob=0.87)\n",
    "hpd\n",
    "abline(v=hpd, lty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common strategy to obtain confidence intervals is to choose the interval where the probability\n",
    "of being below it is as likely as being above it.\n",
    "For instance, if $a=1-0.87$ assuming a $87\\%$ HPD, then the _equal-tailed interval_\n",
    "corresponds to the $\\{a/2,1-a/2\\}$- quantiles of the distribution.\n",
    "This interval includes the median and, if the distribution is symmetric,\n",
    "both credible intervals will be the same.\n",
    "We can easily calculate such quantiles in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- 1-0.87\n",
    "eqi <- quantile(x, probs=c( a/2, 1-(a/2) ) )\n",
    "plot(x=theta, y=posterior, xlab=expression(theta), ylab=\"Posterior density\", type=\"l\")\n",
    "abline(v=eqi, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we summarise our results?\n",
    "Typically, we can report:\n",
    "* the posterior mean,\n",
    "* several posterior percentiles (e.g. 0.025, 0.25, 0.50, 0.75, 0.975),\n",
    "* a credible interval,\n",
    "* posterior probabilities $p(\\theta>c|y)$ where $c$ is a notable point (e.g. 0, 1, depending on the problem),\n",
    "* a plot of the distribution to check whether it is unimodal, multimodal, skewed, etc etc.\n",
    "\n",
    "### Hypothesis testing\n",
    "\n",
    "The frequentist approach to compare predictions made by alternative scientific explanations is based on classic ideas of Fisher, Neyman and Pearson.\n",
    "Typically, one formulates a null hypothesis $H_0$ and an alternative hypothesis $H_a$.\n",
    "Then an appropriate test statistic is chosen $T({Y})$.\n",
    "Finally, one computes the _observed significance_, or _p_-value, of the test\n",
    "as the chance that $T({Y})$ is \"more extreme\" that $T(y_{obs})$, where the \"extremeness\"\n",
    "is towards the alternate hypothesis.\n",
    "If the _p_-value is less than some threshold, typically in the form of a pre-specified\n",
    "Type I error rate, $H_0$ is rejected, otherwise it is not.\n",
    "\n",
    "While widely used, there are few criticisms to this approach:\n",
    "* it is applied only when two hypotheses are nested, one within the other; typically, $H_0$ is a simplification of $H_a$ and involves setting one parameter of $H_a$ to some known constant value;\n",
    "* it offers evidence _against_ the null hypothesis; a large _p_-value does not mean that the two models are equivalent, but only that we lack evidence of the contrary; we don't \"accept the null hypothesis\" but \"fail to reject it\";\n",
    "* a _p_-value does not offer a direct interpretation in terms of weight of evidence but only as a long-term probability (a _p_-value is not the probability that $H_0$ is true!).\n",
    "\n",
    "The Bayesian approach to hypothesis testing is simpler and more intuitive.\n",
    "Basically, one calculates the posterior probability that the first hypothesis is correct.\n",
    "In general, one can test as many models as desired, $M_i, i=1,...,m$.\n",
    "\n",
    "Suppose we have two models $M_1$ and $M_2$ for data $Y$ and the two models have\n",
    "parameters ${\\theta}_1$ and ${\\theta}_2$.\n",
    "With prior densities $\\pi_i({\\theta}_i)$, with $i=1,2$, the marginal distributions of $Y$ are\n",
    "\\begin{equation}\n",
    "    P(y|M_i) = \\int f(y|\\theta_i,M_i) \\pi_i(\\theta_i) d\\theta_i\n",
    "\\end{equation}\n",
    "Then the Bayes' theorem can be used to calculate the posterior probabilities $P(M_1|y)$ and $P(M_2|y)=1-P(M_1|y)$ for the two models.\n",
    "\n",
    "A _Bayes factor_ (BF) is used to summarise these results, and it is equal to the ratio of\n",
    "posterior odds of $M_1$ to the prior odds of $M_1$\n",
    "\\begin{equation}\n",
    "    BF = \\frac{P(M_1|y)/P(M_2|y)}{P(M_1)/P(M_2)}=\\frac{p(y|M_1)}{p(y|M_2)}\n",
    "\\end{equation}\n",
    "This equation turns out to be the ratio of observed marginal densities for the two models\n",
    "(note that $P(M_i)$ is the prior probability).\n",
    "If the two models are _a priori_ equally probable, then\n",
    "\\begin{equation}\n",
    "    BF = p(M_1|y) / p(M_2|y)\n",
    "\\end{equation}\n",
    "which is the posterior odds of $M_1$.\n",
    "\n",
    "The interpretation of BF is that it captures the change in the odds in favour of model 1 as we move from the prior to the posterior, as summarised in the following table.\n",
    "\n",
    "| BF | Strength of evidence|\n",
    "| --- | -------- |\n",
    "| 1 to 3 | not worth more than a bare mention|\n",
    "| 3 to 20 | positive|\n",
    "| 20 to 150 | strong|\n",
    "| >150 | very strong|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Practical: quantifying population variation\n",
    "\n",
    "We now suppose that we sequenced several genomes of bears and, using the method in the previous practical,\n",
    "assigned each individual genotype.\n",
    "We now address a further question: what is the frequency of a certain allele at the population level?\n",
    "Be aware that we have only a sample of the entire population of bears but we want to make\n",
    "inferences at the whole population level.\n",
    "![](Images/BrownBear.jpg)\n",
    "\n",
    "Our sample contains information for 100 individuals with the following genotypes: 63 AA, 34 AG, 3 GG.\n",
    "A frequentist estimate of the frequency of G is given by: $(34+(3*2))/200=40/200=0.20$.\n",
    "What is the posterior distribution for the population frequency of G?\n",
    "\n",
    "The first thing we need to do is define our likelihood model.\n",
    "We can imagine to randomly sample one allele from the population and each time the allele\n",
    "can be either G or not.\n",
    "This is a set of Bernoulli trials and we can use of Binomial distribution as likelihood function.\n",
    "\n",
    "The Binomial likelihood is\n",
    "\\begin{equation}\n",
    "     P(k|p,n) = ( \\genfrac{}{}{0pt}{}{n}{k} ) p^k(1-p)^{n-k}\n",
    "\\end{equation}\n",
    "where $k$ is the number of successes (i.e. the event of sampling a G), $p$ is the proportion\n",
    "of $G$ alleles we have (i.e. the probability of a success), and $n$ is the number of alleles\n",
    "we sample.\n",
    "Recall that\n",
    "\\begin{equation}\n",
    "    (\\genfrac{}{}{0pt}{}{n}{k}) = \\frac{n!}{k!(n-k)!}\n",
    "\\end{equation}\n",
    "Note that the combinatorial term does not contain $p$.\n",
    "\n",
    "What is the maximum likelihood estimate of $p$?\n",
    "You may recall that it is $\\hat{p}=\\frac{k}{n}$.\n",
    "Note that the combinatorial terms does not affect this estimate.\n",
    "\n",
    "The second thing we need to do is define a prior probability for $p$.\n",
    "What is the interval of values that $p$ can take?\n",
    "It is $[0,1]$, as we express frequencies relative to the whole population/sample.\n",
    "It is convenient to choose a prior distribution which is conjugate to the Binomial.\n",
    "A Beta distribution is a conjugate prior in this case.\n",
    "\n",
    "Are certain values of $p$ more likely to occur without observing the data?\n",
    "If we assume that it is not the case, can we use the Beta distribution to generate\n",
    "a noninformative prior?\n",
    "We can choose $Beta(\\alpha=1,\\beta=1)$, which is defined as\n",
    "\\begin{equation}\n",
    "    P(p) = \\frac{1}{B(\\alpha,\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "\\end{equation}\n",
    "where $\\frac{1}{B(\\alpha,\\beta)}$ is simply a normalisation term which does not depend on $p$.\n",
    "\n",
    "The full model can be expressed as $P(p|k,n) \\approx P(k|p,n)P(p)$.\n",
    "What is the closed form for the posterior distribution given our choices for the\n",
    "likelihood and prior functions?\n",
    "It is\n",
    "\\begin{equation}\n",
    "    P(p|k,n) \\approx p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\n",
    "\\end{equation}.\n",
    "The posterior distribution (Beta-Binomial model) is a Beta distribution with parameters $k+\\alpha$ and $n-k+\\beta$.\n",
    "If we set $\\alpha=\\beta=1$ then $P(p|k,n)=Beta(k+1,n-k+1)$.\n",
    "What is $k$ and $n$ in this example?\n",
    "$n$ is the number of alleles we sample and $k$ is the occurence of allele $G$ in our sample.\n",
    "\n",
    "##### A) \n",
    "Plot this posterior probability in R. Then calculate the maximum a posteriori value, 95\\% credible intervals, and notable quantiles. What happens to the distribution if we have only 10 samples (with the sample allele frequency of 0.20)?\n",
    "\n",
    "Now let's think of a more informative prior.\n",
    "The genome-wide distribution of allele frequencies for human populations as a particular shape. This is called a site frequency spectrum (SFS) or allele frequency spectrum (AFS).\n",
    "![](Images/AFS.png)\n",
    "\n",
    "We can have another view at it by plotting the minor allele counts (MAC) distribution.\n",
    "![](Images/MAC.png)\n",
    "\n",
    "Does this distribution fit with a uniform prior?\n",
    "Can we use a conjugate (Beta) function to model this distribution?\n",
    "For instance, choosing $\\alpha=0.5$ and $\\beta=2$ will put more weights on low-frequency variants.\n",
    "However, we don't know _a priori_ whether the allele we are interested in is the minor allele.\n",
    "Therefore a prior distribution with more density at both low and high frequencies might be more appropriate.\n",
    "For instance, this could be achieved by setting $\\alpha=0.1$ and $\\beta=0.1$.\n",
    "\n",
    "##### B) \n",
    "Recalculate the posterior distribution of $p$ using an informative prior (make your own\n",
    "choices regarding the parameter for the Beta distribution) both in the case of 100 and 10 samples.\n",
    "Discuss how these results compare to the previous ones obtained in point A.\n",
    "\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian computation\n",
    "\n",
    "As previously discussed, the calculation of posterior distributions often\n",
    "involves the evaluation of complex high-dimensional integrals (e.g. the denominator of Bayes' formula).\n",
    "This is particularly true when a conjugate prior is not available or appropriate.\n",
    "The two ways of addressing this issue are through (i) asymptotic methods for approximating the posterior density and (ii) numerical integration.\n",
    "\n",
    "### Asymptotic methods\n",
    "\n",
    "When there are many data points, the likelihood will be peaked and the posterior\n",
    "distribution will not be affected by the prior much.\n",
    "Therefore, small changes in the prior will have little effect on the posterior and the likelihood will be concentrated in a small region.\n",
    "\n",
    "Let's recall again the case of the Beta-Binomial model of population allele frequencies.\n",
    "If we have more data (sampled alleles) then the posterior (and the likelihood) will be\n",
    "skewer. This can be easily visualised using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta-Binomial model of population allele frequencies\n",
    "p <- seq(0, 1, 0.01)\n",
    "\n",
    "# 1000 chromosomes (200 G alleles)\n",
    "k <- 200\n",
    "n <- 1000\n",
    "alpha <- k+1\n",
    "beta <- n-k+1\n",
    "plot(x=p, y=dbeta(p, shape1=alpha, shape2=beta), ylab=\"Posterior density\" , xlab=\"Population frequency of G\", type=\"l\")\n",
    "\n",
    "# 100 chromosomes (20 G alleles)\n",
    "k <- 20\n",
    "n <- 100\n",
    "alpha <- k+1\n",
    "beta <- n-k+1\n",
    "points(x=p, y=dbeta(p, shape1=alpha, shape2=beta), type=\"l\", lty=2)\n",
    "\n",
    "# 10 chromosomes (2 G alleles)\n",
    "k <- 2\n",
    "n <- 10\n",
    "alpha <- k+1\n",
    "beta <- n-k+1\n",
    "points(x=p, y=dbeta(p, shape1=alpha, shape2=beta), type=\"l\", lty=3)\n",
    "\n",
    "legend(\"topright\", legend=c(1e3,1e2,1e1), lty=1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scenario where many data points are available, $P(\\theta|y)$ will be approximately\n",
    "Normally distributed.\n",
    "This is given by the _Bayesian Central Limit Theorem_.\n",
    "More formally, for large data points $n$ the posterior can be approximated by a\n",
    "Normal distribution with mean equal to the posterior mode and (co)variance (matrix)\n",
    "equal to minus the inverse of the second derivative matrix of the log posterior evaluated\n",
    "at the mode.\n",
    "\n",
    "For instance, recalling our Beta-Binomial model for population allele frequencies,\n",
    "if we use a flat Beta prior, we have $P(\\theta|x) \\approx \\theta^x(1-\\theta)^{n-x}$.\n",
    "The Normal approximation follows the procedure:\n",
    "* take the log: $l(\\theta)=x\\log\\theta+(n-x)\\log(1-\\theta)$,\n",
    "* take the derivative of $l(\\theta)$ and set it to zero, obtaining $\\hat{\\theta}^\\pi=\\frac{x}{n}$,\n",
    "* take the second derivative evaluated at $\\hat{\\theta}$, $-\\frac{n}{\\hat{\\theta}}-\\frac{n}{1-\\hat{\\theta}}$,\n",
    "* take the minus inverse, $\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}$,\n",
    "* $P(\\theta|x) \\sim N(\\hat{\\theta}^\\pi, \\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n})$.\n",
    "Remember that the derivative of $log(x)$ is $1/x$ and that the derivative of $log(1-x)$ is $1/(x-1)$.\n",
    "\n",
    "Let's plot the exact posterior distribution and the Normal approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta-Binomial model of population allele frequencies\n",
    "p <- seq(0, 1, 0.01)\n",
    "\n",
    "# 100 chromosomes (20 G alleles)\n",
    "k <- 20\n",
    "n <- 100\n",
    "\n",
    "# exact posterior with flat prior\n",
    "alpha <- k+1\n",
    "beta <- n-k+1\n",
    "plot(x=p, y=dbeta(p, shape1=alpha, shape2=beta), ylab=\"Density\", xlab=\"Population frequency\", type=\"l\")\n",
    "\n",
    "# Normal approximation\n",
    "thetaHat <- k/n\n",
    "var <- thetaHat*(1-thetaHat)/n\n",
    "points(x=p, y=dnorm(p, mean=thetaHat, sd=sqrt(var)), type=\"l\", lty=2)\n",
    "\n",
    "legend(\"topright\", c(\"exact\",\"approx\"), lty=1:2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the modes are very similar but the approximated curve fails to capture the asymmetry of the tails.\n",
    "\n",
    "Other normal approximations are used.\n",
    "If the prior is flat, then we can replace the posterior mean by the MLE.\n",
    "Alternatively, we can replace the posterior mode by the posterior mean.\n",
    "These approximations are called _model approximations_ or _first order approximations_ as they estimate $\\theta$ by the mode and the error goes to $0$ at a rate proportional to $1/n$.\n",
    "\n",
    "Estimates of posterior moments and quantiles can be obtained simply as the corresponding features of the approximated Normal density.\n",
    "However, the estimates of moments and quantiles may be poor if the posterior\n",
    "differs from normality.\n",
    "_Laplace's Method_ provides a second order approximation to the posterior mean, with an error that decreases at a rate $1/n^2$.\n",
    "\n",
    "In the previous example, we can easily compare the exact and first order approximated quantiles in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(signif=2)\n",
    "\n",
    "# data\n",
    "k <- 20\n",
    "n <- 100\n",
    "\n",
    "# exact posterior with flat prior\n",
    "alpha <- k+1\n",
    "beta <- n-k+1\n",
    "exact <- rbeta(1e5, shape1=alpha, shape2=beta)\n",
    "signif(quantile(exact),3)\n",
    "\n",
    "# Normal approximation\n",
    "thetaHat <- k/n\n",
    "var <- thetaHat*(1-thetaHat)/n\n",
    "approx <- rnorm(1e5, mean=thetaHat, sd=sqrt(var))\n",
    "signif(quantile(approx),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of asymptotic methods are:\n",
    "* they replace numerical integration with numerical differentiation,\n",
    "* they are deterministic (without elements of stochasticity),\n",
    "* they reduce the computational complexity if any study of robustness (how sensitive are our conclusions to changes in the prior/likelihood?).\n",
    "\n",
    "Asymptotic methods have also disadvantages as\n",
    "* they require that the posterior is unimodal,\n",
    "* they require that the size of the data is large (how large is \"large enough\"?),\n",
    "* for high high-dimensional parameters the calculation of Hessian matrices (second derivatives) is hard.\n",
    "Researchers now use iterative methods based on Monte Carlo sampling, which are longer to run but more general and relatively easy to implement.\n",
    "\n",
    "### Non-iterative Monte Carlo methods\n",
    "\n",
    "Direct sampling of the posterior density can be done using a Monte Carlo integration.\n",
    "Suppose that ${\\theta} \\sim h({\\theta})$ with $h({\\theta})$ being a posterior distribution.\n",
    "Again, $\\theta$ can be a vector of parameters.\n",
    "We aim to estimate $\\gamma$, the posterior mean of $c({\\theta})$, with $\\gamma \\equiv \\mathrm{E}[c({\\theta})] = \\int c({\\theta}) h({\\theta}) d{\\theta}$.\n",
    "If ${\\theta}_1, {\\theta}_2, ..., {\\theta}_N$ are independent and identically distributed (iid) \n",
    "as $h({\\theta})$, then\n",
    "\\begin{equation}\n",
    "    \\hat{\\gamma}=\\frac{1}{N}\\sum_{i=1}^N c(\\theta_i)\n",
    "\\end{equation}\n",
    "which converges to $\\mathrm{E}[c({\\theta})]$ with probability 1 as $N \\rightarrow \\infty$.\n",
    "The computation of posterior expectations requires only a sample of size $N$ from\n",
    "the posterior distribution.\n",
    "\n",
    "In contrast to asymptotic methods, accuracy improves with $N$ the Monte Carlo sample\n",
    "size (which we can choose and have control upon) rather than $n$ the size of the data\n",
    "set (which can may not be able to control).\n",
    "With higher dimensionality of ${\\theta}$, more samples are needed but the structure\n",
    "remains the same.\n",
    "\n",
    "The variance of $\\hat{\\gamma}$ can be estimated from the sample variance of the\n",
    "$c({\\theta}_i)$ values.\n",
    "The standard error estimate for $\\hat{\\gamma}$ is\n",
    "\\begin{equation}\n",
    "    \\hat{se}(\\hat{\\gamma}) = \\sqrt[]{ \\frac{1}{N(N-1)} \\sum_{i=1}^N [c({\\theta_i})-\\hat{\\gamma}]^2 }\n",
    "\\end{equation}\n",
    "The Central Limit Theorem implies that $\\hat{\\gamma} \\pm 2 \\hat{se}(\\hat{\\gamma})$\n",
    "provides the approximated $95\\%$ confidence interval.\n",
    "$N$ can be chosen as large as necessary to provide a desirable confidence interval.\n",
    "\n",
    "In the univariate case, a histogram of the sampled $\\theta_i$ estimates the posterior itself,\n",
    "as the probability of each bin converges to the true bin probability.\n",
    "Indeed, an estimate of $p \\equiv P\\{a<c(\\theta)<b\\}$ is given by\n",
    "\\begin{equation}\n",
    "    \\hat{p} = \\frac{\\text{number of } c(\\theta_i) \\in (a,b)}{N}\n",
    "\\end{equation}\n",
    "We can use a kernel density function to smooth the histogram, using a Normal or\n",
    "rectangular distribution.\n",
    "As previsouly discussed, given a sample from the posterior distribution, almost\n",
    "any quantity can be estimated.\n",
    "\n",
    "What happens if we can't directly sample from this distribution?\n",
    "There are methods for _indirect_ sampling of the posterior distribution.\n",
    "The most commonly used ones are (i) importance sampling, (ii) rejection sampling, (iii) weighted bootstrap.\n",
    "In the importance sampling, an importance function is derived to approximate the\n",
    "Normalised likelihood times the prior, and a weight function is then used for sampling.\n",
    "In the weighted bootstrap, instead of resampling from the set $\\{\\theta_1,...,\\theta_N\\}$\n",
    "with equal probabilities, some points are sampled more often than others because of\n",
    "unequal weighting.\n",
    "\n",
    "In the _rejection sampling_, a smooth density called the envelope function to\n",
    "\"cover\" rather than approximate the posterior distribution.\n",
    "Suppose we can identify an _envelope function_ $g({\\theta})$ and a constant $M>0$\n",
    "such that $L({\\theta})\\pi({\\theta})<Mg({\\theta})$ for all ${\\theta}$.\n",
    "Then the algorithm, in the univariate case, is the following:\n",
    "* generate $\\theta_i \\sim g(\\theta)$,\n",
    "* generate $U \\sim Uniform(0,1)$,\n",
    "* if $MUg(\\theta_i)<L(\\theta_i)\\pi(\\theta_i)$ accept $\\theta_i$ otherwise reject $\\theta_i$,\n",
    "* repeat this procedure until $N$ samples are obtained.\n",
    "The members of this sample will be random variables from $h(\\theta)$.\n",
    "\n",
    "The intuition for the rejection sampling algorithm is given in the figure below.\n",
    "The idea behind is that it is typically hard to sample from the true posterior but it is\n",
    "much easier to sample from the envelope function.\n",
    "![](Images/Rejection.png)\n",
    "\n",
    "Let's make a simple example in R to approximate a Beta distribution using\n",
    "a uniform envelope function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate density from true posterior\n",
    "calcTrueDensity <- function(x) dbeta(x, 3, 10)\n",
    "\n",
    "# g is a uniform prior, M is the maximum density value of the posterior (if known)\n",
    "x <- seq(0,1,0.01)\n",
    "epsilon <- 1e-3\n",
    "M <- max(calcTrueDensity(x)) + epsilon\n",
    "\n",
    "thetas <- c()\n",
    "\n",
    "# we want N samples\n",
    "N <- 1e4\n",
    "rawDensity <- rbeta(N, 3, 10)\n",
    "\n",
    "while (length(thetas) < N) {\n",
    "\n",
    "    theta_j <- runif(1, 0, 1)\n",
    "    U <- runif(1, 0, 1)\n",
    "\n",
    "    if (M*U < calcTrueDensity(theta_j)) thetas <- c(thetas, theta_j)\n",
    "\n",
    "}\n",
    "\n",
    "# check with a qq-plot\n",
    "qqplot(rawDensity, thetas)\n",
    "abline(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chain Monte Carlo methods\n",
    "\n",
    "All previous methods are non-iterative as they draw a sample of fixed size $N$.\n",
    "There is no notion of \"convergence\" in these methods but rather we require $N$\n",
    "to be sufficiently large.\n",
    "For many problems with high dimensionality it may be difficult to find an importance\n",
    "sampling density that, for instance, is acceptable to approximate the (log) posterior\n",
    "distribution.\n",
    "\n",
    "In these cases, it is standard practice to use _Markov chain Monte Carlo_ (MCMC) methods.\n",
    "The rationale is that these methods can sequentially sample parameter values from a\n",
    "Markov chain whose stationary distribution is the desired posterior distribution.\n",
    "\n",
    "A Markov process is a mathematical object following a stochastic (random) process,\n",
    "usually defined as a collection of random variables.\n",
    "A Markov process has the property that the next value of the process depends only on\n",
    "the current value, and it is independent from the previous ones.\n",
    "In other words, the future value will depend only on the current state.\n",
    "A Markov chain is a Markov process that has a particular type of state space,\n",
    "which dictates the possible values that a stochastic process can take.\n",
    "\n",
    "The figure below depicts a diagram of a two-state Markov process, with the states labelled E and A.\n",
    "Each number represents the probability of the Markov process changing from one state to another state, with the direction indicated by the arrow.\n",
    "![](Images/Markov.png)\n",
    "\n",
    "The great increase of generality of MCMC methods comes at the price of requiring an\n",
    "assessment of _convergence_ of the Markov chain to its stationary distribution.\n",
    "The stationary distribution is the probability distribution to which the process converges for large values of steps, or iterations.\n",
    "Convergence is usually assessed using plots or numerical summaries of the sampled distribution from the chain.\n",
    "The majority of Bayesian MCMC computation is based on two algorithms: the _Gibbs sampler_\n",
    "and the _Metropolis-Hastings_ (M-H) algorithm.\n",
    "\n",
    "#### Gibbs sampler\n",
    "\n",
    "Let's suppose that our model has $k$ parameters so that $\\vec{\\theta}=(\\theta_1, \\theta_2, ..., \\theta_k)$.\n",
    "We also ssume that we can sample from the full conditional distributions.\n",
    "The collection of full conditional distributions uniquely determines the joint\n",
    "posterior distribution $P(\\vec{\\theta},{y})$ and all marginal posterior\n",
    "distributions $P(\\theta_i,{y})$, for $i=1,...,k$.\n",
    "\n",
    "Given an arbitrary set of starting values $\\{\\theta_2^{(0)}, ..., \\theta_2^{(k)}\\}$,\n",
    "for $(t=1, ..., T)$, the algorithm:\n",
    "* draws $\\theta_1^{(t)}$ from $P(\\theta_1 | \\theta_2^{(t-1)}, \\theta_3^{(t-1)}, ..., \\theta_k^{(t-1)}, y )$\n",
    "* draws $\\theta_2^{(t)}$ from $P(\\theta_2 | \\theta_1^{(t)}, \\theta_3^{(t-1)}, ..., \\theta_k^{(t-1)}, y )$\n",
    "* ...\n",
    "* draws $\\theta_k^{(t)}$ from $P(\\theta_k | \\theta_1^{(t)}, \\theta_2^{(t)}, ..., \\theta_{k-1}^{(t)}, y )$\n",
    "\n",
    "Under most conditions, $(\\theta_1^{(t)}, \\theta_2^{(t)}, ..., \\theta_k^{(t)})$ converges to a draw from the true joint posterior distribution $P(\\theta_1, \\theta_2, ..., \\theta_k | y)$.\n",
    "This implies that for sufficiently large $t>t_0$ then $\\{\\theta^{(t)}, t=t_0+1, ..., T\\}$ is a correlated sample from the true posterior distribution.\n",
    "\n",
    "A histogram of $\\{\\theta_i^{(t)}, t=t_0+1, ..., T\\}$ provides an estimate of the marginal posterior distribution for $\\theta_i$.\n",
    "The posterior mean can be estimated as\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathrm{E}}(\\theta_i|y) = \\frac{1}{T-t_0} \\sum_{t=t_0+1}^T \\theta_i^{(t)}\n",
    "\\end{equation}\n",
    "The time $0<=t<=t_0$ is called the _burn-in period_.\n",
    "\n",
    "To speed up the computational time, we can run $m$ chains (typically $m \\leq 5$ and chains run in parallel).\n",
    "This procedure is also useful for assessing convergence.\n",
    "In this case, the posterior mean is estimated as\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathrm{E}}(\\theta_i|y) = \\frac{1}{m(T-t_0)} \\sum_{j=1}^m \\sum_{t=t_0+1}^T \\theta_{i,j}^{(t)}\n",
    "\\end{equation}\n",
    "The entire marginal posterior density of $\\theta_i$ is estimated as\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathrm{E}}(\\theta_i|y) \\approx \\frac{1}{m(T-t_0)} \\sum_{j=1}^m \\sum_{t=t_0+1}^T P(\\theta_{i} | \\theta_{l \\neq i,j}^{(t)}, y) \n",
    "\\end{equation}\n",
    "\n",
    "A minimal requirement for Gibbs convergence is that the parameter space must\n",
    "be fully connected, without \"holes\".\n",
    "Imagine a joint posterior distribution for two univariate parameters $\\theta$ and $\\nu$ with two disconnected regions of support.\n",
    "The posterior is either defined for $(\\theta>0 \\text{ and } \\nu>0$ or for\n",
    "$(\\theta<0 \\text{ and } \\nu<0$.\n",
    "If we choose $\\theta^{(0)}>0$ then we will have $\\mu^{(1)}>0$ and subsequentially\n",
    "$\\theta^{(1)}>0$ and so on.\n",
    "The chain will not be able to \"escape\" the first quadrant.\n",
    "\n",
    "An additional issue appears when $\\theta$ and $\\nu$ are highly correlated as this\n",
    "will lead to autocorrelation.\n",
    "The chain will therefore have a \"slow mixing\" and might get trapped in one part of\n",
    "the joint distribution.\n",
    "A solution is called _thinning_ and relies on retaining only every $m^{th}$ iteratation.\n",
    "If $m$ is large enough, then the samples will be uncorrelated.\n",
    "\n",
    "#### Metropolis-Hastings algorithm\n",
    "\n",
    "To ensure that all the full conditional distributions are available, the prior\n",
    "distribution of each parameter can be chosen to be conjugate to the corresponding likelihood.\n",
    "When the priors and likelihoods are not conjugate pairs, one of more of these conditional probabilities may not be available in closed form.\n",
    "Nevertheless, even under these conditions the full conditional probabilities\n",
    "will be available up to a proportionality constant.\n",
    "\n",
    "The _Metropolis_ algorithm is a rejection algorithm which requires only\n",
    "a function proportional to the distribution to be sampled, at the cost of a\n",
    "rejection step from a candidate density function.\n",
    "In this algorithm we can generate samples from a joint posterior distribution\n",
    "$h(\\vec{\\theta})$ such as $P(\\vec{\\theta}|y) \\propto h(\\vec{\\theta}) \\equiv f(y|\\vec{\\theta})\\pi(\\vec{\\theta})$.\n",
    "\n",
    "The algorithm begins by proposing a _candidate_, or _proposal_, symmetric density $q(\\vec{\\theta}^* | \\vec{\\theta}^{(t-1)})$ which satisfies $q(\\vec{\\theta}^* | \\vec{\\theta}^{(t-1)})=q(\\vec{\\theta}^{(t-1)} | \\vec{\\theta}^*)$.\n",
    "From a starting value $\\vec{\\theta^{(0)}}$ at iteration $t=0$, for $(t=1,..., T)$\n",
    "the algorithm repeats:\n",
    "* draw $\\vec{\\theta}^* = q( \\cdot | \\vec{\\theta}^{(t-1)})$,\n",
    "* calculate $r=h(\\vec{\\theta}^*)/h(\\vec{\\theta}^{(t-1)})$,\n",
    "* if $r \\geq 1$, set $\\vec{\\theta}^{(t)}=\\vec{\\theta}^*$, otherwise\n",
    "..* set $\\vec{\\theta}^{(t)}=\\vec{\\theta}^*$ with probability $r$ or\n",
    "..* set $\\vec{\\theta}^{(t)}=\\vec{\\theta}^{(t-1)}$ with probability $1-r$.\n",
    "Under mild assumptions, $\\vec{\\theta}^{(t)}$ converges in distribution to a draw\n",
    "from the true posterior density $p(\\vec{\\theta}|y)$.\n",
    "\n",
    "The Metropolis algorithm is flexible in the selection of the candidate density $q$\n",
    "but may be less efficient than the Gibbs sampler when not properly tuned.\n",
    "The typical approach is to set the candidate density as:\n",
    "\\begin{equation}\n",
    "    q(\\vec{\\theta}^*|\\vec{\\theta}^{(t-1)}) = N(\\vec{\\theta}^*|\\vec{\\theta}^{(t-1)}, \\tilde{\\Sigma})\n",
    "\\end{equation}\n",
    "This distribution is symmetric and is \"self-correcting\" as candidates are always\n",
    "centered around the current value of the chain.\n",
    "As such, this approach is also called _random walk Metropolis_.\n",
    "\n",
    "The posterior variance is represented by $\\tilde{\\Sigma}$ which can be empirically\n",
    "estimated from a preliminary run.\n",
    "A skewed $q$ density will increase the acceptance rate but also generate more\n",
    "autocorrelated samples and, therefore. it may explore only a small proportion of the parameter space.\n",
    "A rule of thumb is to choose $\\tilde{\\Sigma}$ so that around $50\\%$ of the\n",
    "candidates are accepted.\n",
    "However, often the choice of $\\tilde{\\Sigma}$ is done \\textit{adaptively}.\n",
    "Indeed, one can keep track of the proportion of accepted candidates and\n",
    "tune $\\tilde{\\Sigma}$ accordingly.\n",
    "This is usually done during the burn-in period and it is called _pilot adaptation_.\n",
    "\n",
    "A generalisation of the Metropolis algorithm drops the requirement that the candidate density must be symmetric.\n",
    "For instance, for bounded parameter spaces (e.g. $\\theta>0$) a Gaussian density is not appropriate.\n",
    "In the _Metropolis-Hastings_ algorithm when $q(\\vec{\\theta}*|\\vec{\\theta}^{(t-1)}) \\neq q(\\vec{\\theta}^{(t-1)}|\\vec{\\theta}*)$ we replace the acceptance rate $r$ by\n",
    "\\begin{equation}\n",
    "    r = \\frac{ h(\\vec{\\theta}^*) q(\\vec{\\theta}^{(t-1)}|\\vec{\\theta}^*) }{ h(\\vec{\\theta}^{(t-1)})  q(\\vec{\\theta}^*|\\vec{\\theta}^{(t-1)})  }\n",
    "\\end{equation}\n",
    "Again, a draw $\\vec{\\theta^(t)}$ converges in distribution to a draw from the\n",
    "true posterior density as $t \\rightarrow \\infty$.\n",
    "\n",
    "An alternative approach is to set $q(\\vec{\\theta}^*|\\vec{\\theta}^{(t-1)})=q(\\vec{\\theta}^*)$, \n",
    "where the proposal ignores the current value of the variable.\n",
    "This algorithm is called _Hastings independence chain_.\n",
    "In this case, the acceptance rate becomes\n",
    "\\begin{equation}\n",
    "    r = \\frac{h(\\vec{\\theta}^*)/q(\\vec{\\theta}^*)}{h(\\vec{\\theta}^{(t-1)})/q(\\vec{\\theta}^{(t-1)})}\n",
    "\\end{equation}\n",
    "which is the weight function in the importance sampling.\n",
    "\n",
    "There are many variants and types of MCMC algorithms.\n",
    "In the _Langevin-Hastings_ algorithm we introduce a systematic drift in the candidate density.\n",
    "Another approach is to use auxiliary variables to expand the parameter space, as in the _slice sampler_ algorithm.\n",
    "An appropriate enlargement of the parameter space can broaden the class of distributions\n",
    "we can sample and accelerate convergence.\n",
    "A nice feature of MCMC algorithms is that they can be combined into a single problem\n",
    "using _hybrid_ forms, resulting in a mixture of algorithms.\n",
    "Finally, _adaptive_ algorithms attempt to use the early output from a chain to refine and improve the sampling as it progresses.\n",
    "\n",
    "##### Convergence\n",
    "\n",
    "Convergence is an important issue for MCMC algorithms as their output is random and autocorrelated.\n",
    "When the output is safely thought to come from a true stationary distribution of\n",
    "the Markov chain for all $t>T$ then the MCMC algorithm has converged at time $T$.\n",
    "There are both theoretical basis and diagnostic tools to assess whether the chain has indeed reached convergence.\n",
    "\n",
    "A possible diagnostic strategy is to:\n",
    "* run few parallel chains with starting points drawn from an overdispersed (wide) distribution with respect to the stationary distribution;\n",
    "* visually inspect these chains on a common graph for each parameter (as shown in the figure below);\n",
    "* for each graph calculate the scale reduction factor (to check whether the variation within chains are approximately equal to the total variation);\n",
    "* investigate crosscorrelations among parameters suspected of being confounded.\n",
    "![](Images/Chains.png)\n",
    "\n",
    "##### Software\n",
    "\n",
    "There are several software that implement MCMC algorithms for generating samples from posterior distributions.\n",
    "A commonly used program is OpenBUGS, which is the free and open-source version of WinBUGS.\n",
    "The BRugs package in R calls WinBUGS.\n",
    "Finally, JAGS and its R interface rjags are valid alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Bayesian Computation\n",
    "\n",
    "Bayesian computation involves modelling the joint density of parameters values $\\theta$ and data $x$.\n",
    "The aim is then to compute the posterior probability\n",
    "\\begin{equation}\n",
    "    P(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{m(x)}\n",
    "\\end{equation}\n",
    "which can be difficult as the marginal likelihood\n",
    "\\begin{equation}\n",
    "    m(x) = \\int f(x|\\theta)\\pi(\\theta)d\\theta\n",
    "\\end{equation}\n",
    "can involve a high dimensional integral.\n",
    "If the likelihood can be evaluated up to a normalising constant, Monte Carlo methods can be used to sample from the posterior.\n",
    "\n",
    "As the models become more complicated, the likelihood function becomes difficult to define and compute.\n",
    "Under these circumstances it is easier to _simulate_ data samples from the model given the value of a parameter.\n",
    "If the data are discrete and of low dimensionality, then it is possible to sample from the posterior density of the parameter without an explicit likelihood function and without approximation.\n",
    "\n",
    "### Rejection algorithm\n",
    "\n",
    "Under the discrete case and low dimensionality, the algorithm to sample from the posterior from simulations is the following.\n",
    "Given observation $y$, repeat the following until $N$ points have been accepted:\n",
    "* Draw $\\theta_i \\sim \\pi(\\theta)$\n",
    "* Simulate $x_i \\sim f(x|\\theta_i)$\n",
    "* Reject $\\theta_i$ if $x_i \\neq y$\n",
    "These are sampled from $P(\\theta|x)$.\n",
    "\n",
    "The posterior distribution gives the probability distribution of the parameter values that gave rise to the observations.\n",
    "To calculate summaries of this distribution it is possible to draw a histogram and derive notable percentiles.\n",
    "\n",
    "Let's use again the example of elephant herds.\n",
    "Suppose we observe $4$ herds arriving, so $y=4$.\n",
    "Recalling how we modelled this process, the likelihood function was Poisson-distributed with a Gamma-shaped prior $G(3,1)$.\n",
    "The posterior distribution was Gamma distributed with shape parameter $3+4=7$ and scale $0.5$.\n",
    "Let's assume that we don't know the posterior distribution as we cannot derive it.\n",
    "However, we assume that we know how to simulate $y$ given a certain value of our parameter $\\theta$, the average arrival of herds per day.\n",
    "\n",
    "Let's write some R code to sample from the posterior in a likelihood-free way.\n",
    "You need to source some R functions in the Data folder to perform simulations first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>2.5%</dt>\n",
       "\t\t<dd>1.61607461050153</dd>\n",
       "\t<dt>25%</dt>\n",
       "\t\t<dd>3.38704109191895</dd>\n",
       "\t<dt>50%</dt>\n",
       "\t\t<dd>4.68384738080204</dd>\n",
       "\t<dt>75%</dt>\n",
       "\t\t<dd>6.2685907445848</dd>\n",
       "\t<dt>97.5%</dt>\n",
       "\t\t<dd>10.1489826198667</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[2.5\\textbackslash{}\\%] 1.61607461050153\n",
       "\\item[25\\textbackslash{}\\%] 3.38704109191895\n",
       "\\item[50\\textbackslash{}\\%] 4.68384738080204\n",
       "\\item[75\\textbackslash{}\\%] 6.2685907445848\n",
       "\\item[97.5\\textbackslash{}\\%] 10.1489826198667\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "2.5%\n",
       ":   1.6160746105015325%\n",
       ":   3.3870410919189550%\n",
       ":   4.6838473808020475%\n",
       ":   6.268590744584897.5%\n",
       ":   10.1489826198667\n",
       "\n"
      ],
      "text/plain": [
       "     2.5%       25%       50%       75%     97.5% \n",
       " 1.616075  3.387041  4.683847  6.268591 10.148983 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC+lBMVEUAAAABAQECAgIDAwME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\nw79h0iQAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title “Histogram of thetas”"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load some already prepared functions\n",
    "source(\"Data/abc.R\")\n",
    "\n",
    "# Rejection algorithm\n",
    "N <- 1e4\n",
    "y <- 4\n",
    "\n",
    "# function to simulate is called \"simulatePoisson\"\n",
    "thetas <- c()\n",
    "while (length(thetas) <= N) {\n",
    "\n",
    "        # 1. draw from prior (continuous, bounded, uniform)\n",
    "        theta <- runif(1,0,20)\n",
    "\n",
    "        # 2. simulate observations\n",
    "        ysim <- simulatePoisson(theta)\n",
    "\n",
    "        # 3. accept/reject\n",
    "        if (ysim == y) thetas <- c(thetas, theta)\n",
    "\n",
    "}\n",
    "hist(thetas)\n",
    "quantile(thetas, c(0.025,0.25,0.5,0.75,0.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume that we don't know the likelihood function but we can simulate data under this unknown function.\n",
    "\n",
    "What happens in the continuous case?\n",
    "If the data are of low dimension we can modify the previous algorithm as it follows:\n",
    "* Draw $\\theta_i \\sim \\pi(\\theta)$\n",
    "* Simulate $x_i \\sim p(x|\\theta_i)$\n",
    "* Reject $\\theta_i$ if $\\rho(x_i,y) > \\epsilon$\n",
    "where $\\rho(\\cdot)$ is a function measuring the distance between simulated and observed points.\n",
    "\n",
    "Let's make a further example, recalling the case of water temperature at Bumpass Hell.\n",
    "$\\theta$ is continuous and according to our previous example it has a prior distribution $U(80,110)$.\n",
    "We assume we don't know the likelihood function (which is normally distributed) but we can simulate observations that are distributed according to it.\n",
    "Finally, we assume we have an observation of the temperature $y=91.3514$.\n",
    "\n",
    "What is $\\rho(\\cdot)$?\n",
    "For instance we can use the Euclidean distance\n",
    "\\begin{equation}\n",
    "    \\rho(x_i, y) = \\sqrt[]{(x_i-y)^2}\n",
    "\\end{equation}\n",
    "Let's write some R code to estimate $\\theta$ using this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 91.26865\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>2.5%</dt>\n",
       "\t\t<dd>84.6583034843206</dd>\n",
       "\t<dt>25%</dt>\n",
       "\t\t<dd>89.2081973841414</dd>\n",
       "\t<dt>50%</dt>\n",
       "\t\t<dd>91.4858682546765</dd>\n",
       "\t<dt>75%</dt>\n",
       "\t\t<dd>93.3908108621836</dd>\n",
       "\t<dt>97.5%</dt>\n",
       "\t\t<dd>97.5261773215607</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[2.5\\textbackslash{}\\%] 84.6583034843206\n",
       "\\item[25\\textbackslash{}\\%] 89.2081973841414\n",
       "\\item[50\\textbackslash{}\\%] 91.4858682546765\n",
       "\\item[75\\textbackslash{}\\%] 93.3908108621836\n",
       "\\item[97.5\\textbackslash{}\\%] 97.5261773215607\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "2.5%\n",
       ":   84.658303484320625%\n",
       ":   89.208197384141450%\n",
       ":   91.485868254676575%\n",
       ":   93.390810862183697.5%\n",
       ":   97.5261773215607\n",
       "\n"
      ],
      "text/plain": [
       "    2.5%      25%      50%      75%    97.5% \n",
       "84.65830 89.20820 91.48587 93.39081 97.52618 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC+lBMVEUAAAABAQECAgIDAwME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\nkAAD/g9d/WRS8hTNBQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title “Histogram of thetas”"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rejection algorithm in the continuous case\n",
    "N <- 1e3\n",
    "y <- 91.3514\n",
    "epsilon <- 1e-1\n",
    "\n",
    "# function to simulate is called \"simulateNormal\"\n",
    "\n",
    "# euclidean distance\n",
    "rho <- function(x,y) sqrt((x-y)^2)\n",
    "\n",
    "thetas <- c()\n",
    "while (length(thetas) <= N) {\n",
    "\n",
    "        # 1. draw from prior (continuous, bounded, uniform)\n",
    "        theta <- runif(1, min=50, max=150)\n",
    "\n",
    "        # 2. simulate observations\n",
    "        ysim <- simulateNormal(theta)\n",
    "\n",
    "        # 3. accept/reject\n",
    "        if (rho(ysim,y)<=epsilon) thetas <- c(thetas, theta)\n",
    "\n",
    "}\n",
    "print(mean(thetas))\n",
    "hist(thetas)\n",
    "quantile(thetas, c(0.025,0.25,0.5,0.75,0.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can appreciate that the more the prior is different from the unknown likelihood function, the lower the acceptance rate.\n",
    "\n",
    "An alternative to choose a value for $\\epsilon$ is to rank all distances and select only a proportion of the lowest ones.\n",
    "In this case one sets the number of simulations to be performed (not the number of accepted simulations) and the proportions of simulations to retain.\n",
    "It is convenient to investigate the distribution of ranked distances to be sure to retain true outliers in the distribution.\n",
    "\n",
    "__TASK__ Recalling the previous example, write some R code to estimate $\\theta$ assuming that\n",
    "* our observations are $Y=\\{91.34, 89.21, 88.98\\}$\n",
    "* $\\theta$ has prior $N(\\mu=90,\\sigma^2=20)$ defined only in $80 \\geq \\theta \\leq 110$\n",
    "* the simulating function is called \"simulateNormal\" in \"Data/abc.R\"\n",
    "* the distance function is $\\rho(x_i, Y)=\\frac{\\sum_{j \\in Y} \\sqrt[]{(x_i-j)^2}}{|Y|}$\n",
    "* the total number of simulations is $10,000$ and we want to accept the lowest $5\\%$ of distance\n",
    "Complete the following tasks:\n",
    "* plot the sampled prior distribution\n",
    "* plot the distribution of ranked distances with indication of $5\\%$ threshold\n",
    "* plot the posterior distribution\n",
    "* calculate notable quantiles and HPD $95\\%$ (using the library `coda` and function `HPDinterval(as.mcmc(x), prob=0.95)`)\n",
    "\n",
    "Finally, what happens if we have more observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejection algorithm with proportions of simulations to accept\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data become high dimensional (e.g. multivariate measurements) then it is necessary to reduce the dimensionality via the use of summary statistics.\n",
    "For instance, the complete genome of many samples has high dimensionality as it may have up to $N*L$ genotypes with $N$ samples and $L$ number of sites per-genome.\n",
    "One can calculate summary statistics $S(y)$ to describe some features of the data (e.g. indexes of genetic diversity in the case of multiple genomes).\n",
    "\n",
    "In this case, the following is the prototype for the rejection-ABC algorithm, where ABC stands for Approximate Bayesian Computation.\n",
    "Given observation $y$, repeat the following until $N$ points have been accepted:\n",
    "* Draw $\\theta_i \\sim \\pi(\\theta)$\n",
    "* Simulate $x_i \\sim p(x|\\theta_i)$\n",
    "* Reject $\\theta_i$ if $\\rho(S(x_i),S(y))>\\epsilon$\n",
    "![](Images/ABC.png)\n",
    "\n",
    "The function $S(\\cdot)$ can be a vector.\n",
    "The choice of summary statistics is considered as mapping from a high dimension to a low dimension.\n",
    "Some information is lost, but with enough summary statistics much of the information is kept.\n",
    "The aim for the summary statistics is to satisfy the Bayes sufficiency\n",
    "\\begin{equation}\n",
    "    P(\\theta|x)=P(\\theta|S(x))\n",
    "\\end{equation}\n",
    "\n",
    "The first example of an ABC approach was introduce by Pritchard et al. (1999).\n",
    "They summarised information for 445 Y-chromosome genes copies at eight microsatellites (and therefore 445 times 8 dimensions) into three numbers.\n",
    "The distance was chosen to be a normalised Chebyshev distance\n",
    "\\begin{equation}\n",
    "    \\max_j | \\frac{S_j(x)}{S_j(y)} - 1|\n",
    "\\end{equation}\n",
    "for $j=1,...,s$ summary statistics.\n",
    "\n",
    "You can clearly see one of the first issues in ABC, related to the curse of the dimensionality when using more than a few summary statistics.\n",
    "If summary statistics are uncorrelated, using the above distance, we will reject many simulations with increasing number of summary statistics.\n",
    "Solutions have been proposed in order to (i) use a wider acceptance tolerance and/or (ii) perform a better sampling from the prior.\n",
    "\n",
    "### Regression-based estimation\n",
    "\n",
    "Another possibility to derive posterior using ABC is based on local linear regression, in order to obtain a potentially wider set of accepted points.\n",
    "The algorithm is the following:\n",
    "* Given observation $y$ repeat the following until $M$ points have been generated:\n",
    "  * Draw $\\theta_i \\sim \\pi(\\theta)$\n",
    "  * Simulate $x_i \\sim f(x|\\theta_i)$\n",
    "* Calculate $S_j(x)$ for all $j$ and $k_j$, the empirical standard deviation of $S_j(x)$\n",
    "* $\\rho(S(x),S(y)):\\sqrt[]{\\sum_{j=1}^s ( \\frac{S_j(x)}{k_j} - \\frac{S_j(y)}{k_j} )^2 }$\n",
    "* Choose tolerance $\\epsilon$ such that the proportion of accepted points $P_\\epsilon=\\frac{N}{M}$\n",
    "* Weight the simulated points $S(x_i)$ using $K_\\epsilon(\\rho(S(x_i),S(y)))$ where\n",
    "\\begin{align}\n",
    "    K_\\epsilon(t) &= \\epsilon^{-1}(1-(t/\\epsilon)^2) & \\text{for } t \\leq \\epsilon \\\\\n",
    "    K_\\epsilon(t) &= 0 & \\text{for } t > \\epsilon\n",
    "\\end{align}\n",
    "* Apply weighted linear regression to the $N$ points that have nonzero weight to obtain an estimate of $\\hat{E}(\\theta|S(x))$\n",
    "* Adjust $\\theta_i^*=\\theta_i-\\hat{E}(\\theta|S(x))+\\hat{E}(\\theta|S(y))$\n",
    "* The $\\theta_i^*$ with weights $K_\\epsilon(\\rho(S(x_i),S(y)))$ are random draws from an approximation of the posterior distribution $P(\\theta|y)$.\n",
    "\n",
    "There are problems with regression-based methods too.\n",
    "When the observed summary statistics lies outside the unknown likelihood distribution (model misspecification), then regression is an extrapolation rather than an interpolation.\n",
    "In these cases posterior draws (after regression adjustments) can be outside the prior range.\n",
    "This problem occurs when the observations lie at the boundaries of the unknown likelihood (called prior-predictive distribution in the ABC context).\n",
    "\n",
    "### MCMC-ABC\n",
    "\n",
    "Another possibility to increase the performance of ABC estimation is to do a better sampling.\n",
    "Indeed, the great majority of simulated parameter values may not give rise to summary statistics that are similar enough to the observed data.\n",
    "Efficiency will be slow as many points will be rejected or given negligible weight.\n",
    "We therefore want a procedure whereby parameters are sampled from a distribution that is closer to the posterior than from the prior.\n",
    "There are two main ways to do this, one via Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) sampling.\n",
    "\n",
    "An MCMC-ABC algorithm starts by initialising sampling $\\theta^{(0)} \\sim \\pi(\\theta)$.\n",
    "Then at iteration $t \\geq 1$:\n",
    "* Simulate $\\theta' \\sim K(\\theta|\\theta^{(t-1)})$ where $K(\\cdot)$ is a proposal distribution that depends on the current value of $\\theta$\n",
    "* Simulate $x \\sim p(x|\\theta')$.\n",
    "* If $\\rho(S(x),S(y))<\\epsilon$ (rejection step),\n",
    "  * $u \\sim U(0,1),$\n",
    "  * if $u \\leq \\pi(\\theta')/\\pi(\\theta^{(t-1)}) \\times K(\\theta^{(t-1)}|\\theta')/K(\\theta'|\\theta^{(t-1)})$, update $\\theta{(t)}=\\theta'$, otherwise $\\theta{(t)}=\\theta^{(t-1)}$;\n",
    "* otherwise $\\theta{(t)}=\\theta^{(t-1)}$.\n",
    "\n",
    "A good proposal distribution should resemble the actual posterior distribution of the parameters.\n",
    "A Normal proposal distribution often works well in practice, centred in $\\theta^{(t-1)}$.\n",
    "This is also called the _jumping_ distribution.\n",
    "In this algorithm, at convergence the average distribution of proposed $\\theta'$ is dominated by the posterior itself.\n",
    "It is also possible to apply any regression-adjustment methods on the MCMC sample to obtain more accurate estimates.\n",
    "Compared to the classic MCMC with likelihoods, this algorithm has higher rejection rate.\n",
    "To circumvent this problem, the tolerance $\\epsilon$ can be initially high and then reduced during the burn-in phase.\n",
    "\n",
    "It was later proposed a method called Sequential Monte Carlo (SMC) for iteratively improving on an ABC approximation.\n",
    "This approach consisted of two main features: (i) weighted resampling from the set of points already drawn and (ii) successive reduction in the tolerance $\\epsilon$.\n",
    "\n",
    "### Model assessment in ABC\n",
    "\n",
    "Classical Bayesian inference can be applied to models and parameters in the ABC framework.\n",
    "\n",
    "#### Model choice\n",
    "\n",
    "Given a series of model $\\mu_1, \\mu_2, ..., \\mu_N$ with prior probabilities $\\sum_i \\pi(\\mu_i)=1$, we can calculate Bayes factors between two models $i$ and $j$ as\n",
    "\\begin{equation}\n",
    "    \\frac{p(\\mu_i|x)}{p(\\mu_j|x)} \\div \\frac{p(\\mu_i)}{p(\\mu_j)}  \n",
    "\\end{equation}\n",
    "\n",
    "Typically, Bayes factors can be computed only if the parameters within the models have priors that integrate to one.\n",
    "Therefore, Bayesian model choice can be strongly affected by the prior.\n",
    "Notably, Bayesian model choice automatically penalised models with many parameters.\n",
    "As such, one does not need to account for different number of parameters between models.\n",
    "\n",
    "#### Hierarchical model\n",
    "\n",
    "ABC can also be adopted in a hierarchical Bayesian model.\n",
    "A potential difficulty here is that summary statistics should capture information from each unit so that the hyperparameters can be well inferred.\n",
    "However, if there are many summary statistics it is unlikely that simulated data will closely match the observations.\n",
    "\n",
    "#### Choice of summary statistics\n",
    "\n",
    "A very much arbitrary area of ABC modelling lies in the choice of summary statistics.\n",
    "In some fields, there is a history of relating summary statistics to model parameters.\n",
    "In general, there is no need of a strong theory relating summary statistics to model parameters.\n",
    "One issue here is the effect of summary statistics on inferences and whether some choices may bias the outcome of model choice.\n",
    "This may happen if chosen summary statistics have little relation to parameters in other models.\n",
    "Typically, some summary statistics may cover some aspects of the model while other statistics may cover different aspects, making the choice of a finite set of informative units problematic.\n",
    "\n",
    "The main idea is that as more summary statistics are used, then they should be jointly sufficient for the likelihood.\n",
    "Also, summary statistics may be correlated to each other and to the parameters.\n",
    "However, the accuracy and stability of ABC decreases rapidly with increasing numbers of summary statistics.\n",
    "![](Images/LucyBlanket.jpg)\n",
    "\n",
    "How can we choose the optimal set of summary statistics?\n",
    "For instance, one could calculate the ratio of posterior density with or without a particular summary statistic.\n",
    "Departures greater than a threshold are suggestive that the excluded summary statistic is important.\n",
    "Alternatively, different summary statistics can be weighted differently according to their correlation with some model parameters.\n",
    "The number of summary statistics can also be reduced via multivariate dimensional scaling (e.g. using partial least-squares or principal component analysis).\n",
    "Finally, summary statistics should be scaled in order to have equal mean and variance, if normally distributed, to avoid putting a different weight to sparser distributions.\n",
    "\n",
    "#### Model validation\n",
    "\n",
    "A very important component of Bayesian modelling is validation and testing.\n",
    "Validation is the assessment of goodness-of-fit of the model and comparing alternative models.\n",
    "In ABC it is essential to distinguish errors due to the approximation from errors caused by the choice of the model.\n",
    "If available, one can compare the results of a simulation with expectations based on the theory.\n",
    "\n",
    "Often the marginal (or joint) distribution of simulated summary statistics are visualised and compared to the corresponding target statistic.\n",
    "If the target is outside, then this could be a problem in the model.\n",
    "This issue does not occur in likelihood-based approaches.\n",
    "\n",
    "A similar test is to compare the observations with the posterior predictive distribution.\n",
    "This can be done by simulating data with parameters drawn randomly from the current posterior distribution.\n",
    "\n",
    "### Applications of ABC in ecology and evolution\n",
    "\n",
    "The initial applications of ABC have been mainly in population genetics, using a rejection or regression algorithm.\n",
    "Later on, a number of other areas in ecology, epidemiology and systems biology have seen an increase in the use ABC.\n",
    "The more recent applications use MCMC or SMC algorithms.\n",
    "\n",
    "In population genetics, the data consists of frequencies of alleles or haplotypes in one or more populations.\n",
    "The goal is usually to estimate the demographic history of populations in terms of changes of population sizes, divergence times, migration rates, and so on.\n",
    "A number of studies on inferring human evolution have been using ABC methods.\n",
    "For instance, Patin et al. (2009) compared different demographic models that explain the genetic differentiation within different African populations.\n",
    "![](Images/Patin1.png)\n",
    "![](Images/Patin2.png)\n",
    "\n",
    "Some features of ecology, epidemiology and systems biology appear to be very similar.\n",
    "Many aspects are captured by systems of partial or ordinary differential equations or stochastic differential equations.\n",
    "Data often consist of time series and/or spatial data.\n",
    "The goal here is to compare between hypothesised models that could explain the observed patterns and to infer parameters.\n",
    "Toni et al. (2009) provide an example using a Lotka-Volterra system on prey-predator dynamics from time series data on abundances.\n",
    "![](Images/Toni.png)\n",
    "\n",
    "ABC has also been used for agent-based models, protein interaction networks, speciation rates under a neutral ecological model, extinction rates from phylogenetic data, epidemiology (e.g. transmission).\n",
    "\n",
    "In conclusion, when a likelihood function is known and can be efficiently evaluated, then there is not advantage to use ABC.\n",
    "When the likelihood function is known but difficult to evaluate in practise, the ABC is a valid alternative.\n",
    "Many scenarios that evolutionary biologists or ecologists are interested to can be generated by simulations, making ABC very appealing.\n",
    "ABC can also be useful for initial exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "#### Practical: estimating divergence time in bears\n",
    "\n",
    "##### Preparation\n",
    "\n",
    "For this practical you need some R packages, namely `coda`, `abc`, `grid`, `maps`, `spam`, `fields`.\n",
    "For plotting purposes you may also want to use `ggplot2`.\n",
    "\n",
    "You will also need the software `ms` to be installed.\n",
    "You can find the executable for linux in `bin/ms`.\n",
    "If it doesn't work, you can compile the source `Software/ms.tar.gz` by\n",
    "`tar -xzvf ms.tar.gz; cd msdir; gcc -o ms ms.c streec.c rand1.c -lm`.\n",
    "Finally you need some data and R functions provided in `Data`.\n",
    "\n",
    "I suggest to copy `functions.R` and `polar.brown.sfs*` in the workspace\n",
    "where you will run this practical without overwriting the repository.\n",
    "If you encounter difficulties please do let me know.\n",
    "You can work in teams for this practical.\n",
    "Actually, I encourage that you team up for this exercise.\n",
    "\n",
    "##### Project\n",
    "\n",
    "In this partical you are going to estimate the divergence (or speciation)\n",
    "time between polar bears and brown bears using genomics data.\n",
    "You will be using Approximate Bayesian Computation methods to\n",
    "inference such time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open R and load all R functions and data needed:\n",
    "source(\"Data/functions.R\")\n",
    "load(\"Data/polar.brown.sfs.Rdata\")\n",
    "\n",
    "# Inspect the objects:\n",
    "ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `polar.brown.sfs` includes the joint (2 dimensions) site frequency\n",
    "spectrum (SFS) between polar bears (on the rows) and brown bears (on the columns).\n",
    "This is based on real genomic data from 18 polar bears and 7 brown bears.\n",
    "The site frequency spectrum is a matrix $N \\times M$ where cell $(i,j)$ reports the number of sites\n",
    "with allele frequency $(i-1)$ in polar bears and $(j-1)$ in brown bears.\n",
    "If you want to see this file type `cat polar.brown.sfs` in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot this spectrum:\n",
    "plot2DSFS(polar.brown.sfs, xlab=\"Polar\", ylab=\"Brown\", main=\"2D-SFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each population has $2n+1$ entries in its spectrum, with $n$ being the number of individuals.\n",
    "The number of chromosomes for each species (bears are diploids, like humans)\n",
    "can be retrieved as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChroms.polar <- nrow(polar.brown.sfs)-1\n",
    "nChroms.polar\n",
    "nChroms.brown <- ncol(polar.brown.sfs)-1\n",
    "nChroms.brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing you need to remember about the site frequency spectrum is that we\n",
    "can easily calculate several summary statistics from it.\n",
    "These summary statistics can be used for inferences in an Approximate Bayesian\n",
    "Computation (ABC) framework.\n",
    "\n",
    "For instance, from the site frequency spectrum, we can easily calculate the\n",
    "number of analysed sites (in this example all sites are polymorphic, and thus\n",
    "variable in our sample), which is simply the sum of all entries in the SFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrSites <- sum(polar.brown.sfs, na.rm=T)\n",
    "nrSites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is important as we will condition the simulations to generate this\n",
    "number of sites for each repetition.\n",
    "In other words, when simulating data we will simulate exactly this number of\n",
    "polymoprhic sites to calculate the site frequency spectrum and all\n",
    "corresponding summary statistics afterwards.\n",
    "\n",
    "I provide a function to easily calculate several summary statistics\n",
    "from a site frequency spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsSummaryStats <- calcSummaryStats(polar.brown.sfs)\n",
    "obsSummaryStats\n",
    "# These are the OBSERVED summary statistics! Keep them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the summary statistics available in this practical and their meaning\n",
    "is the following:\n",
    "* fst: population genetic differentiation; it measures how much species are genetically different; it goes from 0 (identical) to 1 (completely different);\n",
    "* pivar1: genetic diversity of species 1 (polar bears);\n",
    "* pivar2: genetic diversity of species 1 (brown bears);\n",
    "* sing1: number of singletons (sites with frequency equal to 1) for species 1 (polar bears);\n",
    "* sing2: number of singletons (sites with frequency equal to 1) for species 2 (brown bears);\n",
    "* doub1: number of doubletons (sites with frequency equal to 2) for species 1 (polar bears);\n",
    "* doub2: number of doubletons (sites with frequency equal to 1) for species 2 (brown bears);\n",
    "* pef: proportion of sites with equal frequency between polar bears and brown bears;\n",
    "* puf: proportion of sites with unequal frequency between polar bears and brown bears (note that puf=1-pef).\n",
    "\n",
    "It is not important that you understand the significance (if any) of all\n",
    "these summary statistics in an evolutionary context.\n",
    "If interested, a nice review is \"Molecular Signatures of Natural Selection\" by\n",
    "Rasmus NielseN.\n",
    "However, some of these summary statistics might be more informative than others.\n",
    "It is your first goal to understand which summary statistics to keep.\n",
    "\n",
    "The parameter you want to estimate is the divergence time between polar\n",
    "and brown bears (T).\n",
    "You first aim is to performs N simulations of data by drawing from a prior\n",
    "distribution of T and record (separately) the drawn values and the corresponding\n",
    "summary statistics generated by that value of T.\n",
    "\n",
    "You can define how many simulations you want to perform (ideally a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrSimul <- 1e4 # but change this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you should define the prior distribution of our parameter to be estimated,\n",
    "the divergence time T.\n",
    "You can use any distribution you find suitable.\n",
    "However, you may want to consider that a reasonable range of values for T is\n",
    "between 200k and 700k years ago.\n",
    "\n",
    "The function to simulate data (specifically the site frequency spectrum)\n",
    "given values of T (and M, the migration rate) is `simulate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as parameters: T (divergence time), M (migration rate),\n",
    "how many sites to simulate, the directory for `ms` program and the text file\n",
    "in output.\n",
    "This function simulates a joint evolutionary history for both polar and brown\n",
    "bears according to what we know in terms of their respective changes in size.\n",
    "However, you can set when they speciated (T in years ago) and the migration rate (M).\n",
    "(Note that the migration rate is scaled by the reference population size\n",
    "so a reasonable range of M is between 0 and 2.)\n",
    "\n",
    "As an example, assuming T=200k and M=0 the command to simulate data and\n",
    "calculate summary statistics is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, set the path to the \"ms\" software you installed\n",
    "msDir <- \"bin/ms\" # this is my specific case, yours could be different\n",
    "\n",
    "# second, set the name for the output text file\n",
    "fout <- \"ms.txt\" # leave it like here\n",
    "\n",
    "# then we can simulate data:\n",
    "simulate(T=2e5, M=0, nrSites, msDir, fout)\n",
    "\n",
    "# and finally calculate the summary statistics for this simulation \n",
    "#(note that you need to specify the number of chromosomes for the two species)\n",
    "simulatedSFS <- fromMStoSFS(fout, nrSites, nChroms.polar, nChroms.brown)\n",
    "calcSummaryStats(simulatedSFS)\n",
    "\n",
    "# you can even plot the simulated site frequency spectrum\n",
    "plot2DSFS(simulatedSFS, xlab=\"Polar\", ylab=\"Brown\", main=\"simulated 2D-SFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the observed summary statistics 'fst', which measures how different\n",
    "polar and brown bears, in relation to the one calculated simulating T=2e5,\n",
    "can you make some initial (very rough) considerations on the most likely values\n",
    "of T (higher or lower than 200k years ago)?\n",
    "\n",
    "You can use the `abc` package and the `abc` function to calculate the posterior\n",
    "distribution (as well as to compute the distance between observed and expected summary statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"abc\")\n",
    "library(abc)\n",
    "?abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, to perform an ABC analysis you need 3 objects:\n",
    "* target: a vector of the observed summary statistics;\n",
    "* param: a vector, matrix or data frame of the simulated parameter values;\n",
    "* sumstat: a vector, matrix or data frame of the simulated summary statistics.\n",
    "You already have 'target' as it is the vector of observed summary statistics\n",
    "called 'obsSummaryStatistics'.\n",
    "\n",
    "You now have everything to estimate the divergence time.\n",
    "For simplicity assume that $M=0$.\n",
    "Also, you are free to choose a rejection or local-regression method, as specified\n",
    "in the 'abc' function.\n",
    "This is not strictly required, but if you want to explore the estimation\n",
    "of two parameters simultaneously, you can estimate M by defining a prior for it,\n",
    "draw random samples jointly of T and M, calculate summary statistics, and so on.\n",
    "\n",
    "##### Hints\n",
    "\n",
    "Please consider these points carefully when completing the project.\n",
    "* Assess which summary statistics are more or less informative for the parameter estimation (e.g. after a first run of simulations with all parameters, look for correlations between the simulated parameter\n",
    " value and summary statistics).\n",
    "* You can also look for correlations between summary statistics and\n",
    " eventually use only one of the pair if two summary statistics are highly\n",
    "  correlated. If you are a pro, you can also perform a principal component or\n",
    " multidimensional scaling analysis (e.g. with package 'pls') and by using\n",
    " each statistic's loadings, you can create novel uncorrelated summary\n",
    " statistics which are linear combinations of the previous ones (this part\n",
    " is purely suggestive and it is not required to obtain the full score).\n",
    "*  Remember to scale your simulated (jointly with the observed) summary\n",
    " statistics separately, so that the mean is zero and standard deviation is one.\n",
    "*  Generate a plot with the posterior distribution of the parameter of\n",
    " interest. You can also show the chosen prior distribution on the same plot.\n",
    "* Calculate the posterior mean, mode, median and  other notable quantities (e.g. 95\\% HPD interval) to summarise the posterior distribution.\n",
    "* I suggest you to use the 'abc' package in R as it implements\n",
    " the local-linear regression method too. However you can also implement a\n",
    " rejection sampling method yourself, as seen in class.\n",
    "* A useful diagnostic plot to show is the distribution of sampled values\n",
    "  from the prior: do they cover the whole range of the prior (and are they distributed as expected)?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
